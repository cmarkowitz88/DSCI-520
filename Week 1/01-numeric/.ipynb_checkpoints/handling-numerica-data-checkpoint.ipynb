{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521\n",
    "\n",
    "## Chapter 1: Handling Numeric Data in Python\n",
    "\n",
    "### Numeric Python (NumPy)\n",
    "\n",
    "The vast majority of data is numeric in nature. Thus, if we are going to become experts at processing and analyzing data, we are certainly going to need to become extremely familiar with using Python with said numeric data. Perhaps surprisingly, the basic capabilities of a fresh installation of Python with regards to processing and analyzing numeric data are quite limited. The most common solution to this problem is to use the Numeric Python module (NumPy). Using NumPy with Python allows you to turn your scripts into excellent calculators! For example, let's look at calculating an average of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# This is the standard way of importing NumPy\n",
    "import numpy as np\n",
    "\n",
    "first_five_indices = range(5) # range is a built-function that makes integers\n",
    "\n",
    "## computing an average the old-fashioned way\n",
    "average = 0\n",
    "for number in first_five_indices:\n",
    "    average = average + number\n",
    "average = average / float(len(first_five_indices))\n",
    "\n",
    "print(average)\n",
    "\n",
    "## using numpy's mean function\n",
    "print(np.mean(first_five_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, NumPy makes taking the mean of a list of numbers much easier. The same could be said of nearly any mathematical computation you could think of doing! Instead of exhaustively going over each in detail, it's probably best to recommend checking the official NumPy documentation. Anytime you intend to use Python to perform computations involving numerical data, it's essential to first check if NumPy has methods that may assist you. As shown above, you can usually save a lot of work and uncessary code using NumPy! The documentation can be found here: https://docs.scipy.org/doc/numpy/reference/index.html.\n",
    "\n",
    "#### Arrays\n",
    "A feature of NumPy that is certainly worthwhile to cover in more depth is the NumPy array. Hopefully by now we are all familiar with the most ubiquitous Python container for storing several objects (lists). While extremely easy and intuitive to use, lists can be quite slow and limited, particularly for numeric data. The NumPy version of a list is called an array. These are generally much faster to work with (as they are implemented using much faster base code), with a tradeoff being that every item in the array must be of the same data type (Python allows for the elements of a list to be any combination of data types, which is one of the leading contributors to their slow speed). Here's how to create an array in NumPy (usually referred to as an nd-array for n-dimensional array):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "sample = [1, 2, 3]\n",
    "\n",
    "# Can cast a list into an array\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array(sample)\n",
    "\n",
    "print(a)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the above arrays, we can simply use indexing with brackets just like how we index with normal Python lists. This won't work the same way with higher-dimensional arrays (which we will discuss later)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "print(a[0])\n",
    "print(b[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy has several extremely convenient functions for the creation of frequently used types of arrays (here, the (1, 4) tuple is to indicate the shape of the array we want, a 1-dimensional array of size 4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]]\n",
      "[[1. 1. 1. 1.]]\n",
      "[[6 6 6 6]]\n",
      "[[0.28480932 0.43519744 0.24802872 0.9895255 ]]\n"
     ]
    }
   ],
   "source": [
    "# Create an array of all 0s\n",
    "c = np.zeros((1, 4))\n",
    "print(c)\n",
    "\n",
    "# Create an array of all 1s\n",
    "d = np.ones((1, 4))\n",
    "print(d)\n",
    "\n",
    "# Create an array full of a single constant, say 6\n",
    "e = np.full((1, 4), 6)\n",
    "print(e)\n",
    "\n",
    "# Create an array of random numbers\n",
    "f = np.random.random((1, 4))\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the best features of NumPy arrays is they allow you to _broadcast_ operations. Broadcasting allows you to perform an operation on every element in the array all at the same time. This contrasts with having to loop through normal Python lists and applying the operation on each element, one-at-a-time. As an example, let's multiply each element in a list and an array by 10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 20, 30, 40, 50, 60, 70, 80]\n",
      "[10 20 30 40 50 60 70 80]\n"
     ]
    }
   ],
   "source": [
    "lst = [1, 2, 3, 4, 5, 6, 7, 8]\n",
    "temp = []\n",
    "arr = np.array(lst)\n",
    "\n",
    "# List operation\n",
    "for element in lst:\n",
    "    temp.append(element * 10)\n",
    "\n",
    "lst = temp\n",
    "print(lst)\n",
    "\n",
    "# same array operation\n",
    "arr = arr * 10\n",
    "\n",
    "print(arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's absolutely essential to be proficient with using arrays when performing any kind of data science, as data points often contain many features, which are most often represented numerically. Anytime you have a list of numbers associated with each data point, it becomes extremely beneficial to represent said data points using arrays made up of these numbers. To represent a collection of data points in this way, one can combine their respective arrays into the form of a _matrix_. This brings us to the sometimes daunting field of linear algebra!\n",
    "\n",
    "## Introduction to Linear Algebra\n",
    "\n",
    "While calculus education commonly constitutes a large share of seconday mathematics education, linear algebra often gets less attention or is left out entirely. This does not mean it's less valuable in the real world, but quite often the opposite, especially now that we have so much data. Linear algebra forms the basis for a lot of modeling in data science, but more foundationally offers some really good, practical ways to represent and intuit numeric data. Let's put it this way:\n",
    "\n",
    "+ __Linear algebra is _the_ framework for dataset arithmetic__\n",
    "\n",
    "### Vectors\n",
    "\n",
    "We've been talking about arrays a bit. Well, instead of thinking about these just as a list of numbers, we'll start to talk about them as __vectors__. Concretely, vectors are points in some finite-dimensional space. So, you have a spreadsheet with $n$ rows and $m$ columns, you can think of a column as an $n$-dimensional vector, and a row as an $m$-dimensional vector. What does and $n$-dimensional vector look like? Graphically, it's hard to think of a vector with more than $3$ dimensions, so it's not uncommon to provide intuitive examples in only $2$ or $3$ dimensions. In $2$ dimensions, a vector, $v$, is really just an $(x,y)$ point, but is often indicated with a line pointing to it from the origin, $(0,0)$, like:\n",
    "\n",
    "![vectors](img/Vector_components.png)\n",
    "\n",
    "This is done to indicate that vectors have direction.\n",
    "\n",
    "#### Arithmetic\n",
    "\n",
    "Linear algebra _is_ algebra, and comes with generalizations of all of our favorite arithmetic operations. In fact, since we've gotten off the ground last week with columns and rows of data, we've actually been doing vector arithmetic operations. So, what are they and how are they done?\n",
    "\n",
    "#### Vector Norms\n",
    "\n",
    "We've already talked about norms in the context of defining a distance metric, or \"normalizing\" one into a similarity metric. Well, of the most important tools that comes out of a linear-algebra toolkit is a bigness measure, or, norm. For a vector $v = [v_1,v_2, \\cdots,v_n]$, its norm (size) is indicated by $\\|v\\|$, and in Euclidean, i.e., striaght-line-distance space, it's computed as:\n",
    "$$\\|v\\| = \\sqrt{v_1^2 + v_2^2 + \\cdots + v_n^2}$$\n",
    "\n",
    "Previously, we squared the components of a vector, i.e., squared it pointwise, added them up, and found the square-root to produce the norm of a vector. Now, since we're thinking of norms and linear algebra at a high level we'll just use the built-in numpy function, `numpy.linalg.norm()`, which defaults to the euclidean norm. Note: this function also does other norms, like the taxicab norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9. 2. 4. 8. 1.]\n",
      "12.884098726725126\n"
     ]
    }
   ],
   "source": [
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "print(np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition & Subtraction\n",
    "\n",
    "For vectors, addition and subtraction is \"pointwise\" and only works with same-size (dimension) vectors. Pointwise addition between $u = [u_1,u_2, \\cdots,u_n]$ and $v = [v_1,v_2, \\cdots,v_n]$ means the following:\n",
    "$$u + v = [u_1+v_1, u_2+v_2, \\cdots, u_n+v_n]$$\n",
    "Graphically, here's what's going on when you add two vectors together:\n",
    "\n",
    "![uv addition](img/uv-addition.gif)\n",
    "\n",
    "We can do vector addition/subtraction by using numpy. Specifically, when we run `np.array()` on a list the resulting object works like a vector and handles the rest for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   3.7  2.  10.   0. ]\n",
      "[9. 2. 4. 8. 1.]\n",
      "[ 4.   5.7  6.  18.   1. ]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "print(u)\n",
    "\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "print(u + v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication\n",
    "\n",
    "Vector multiplication is less straightforward and comes in flavors. We'll go through these, building up in complexity.\n",
    "\n",
    "__Scalar multiplication just means multiplying a vector by a constant__\n",
    "\n",
    "So, in this case every single value of a vector is multiplied the usual way by the constant value. If we have a vector $v = [v_1, v_2, \\cdots, v_n]$ and a constant, $c$, their product is $$cv = [cv_1, cv_2, \\cdots, cv_n]$$\n",
    "\n",
    "This is specifically called scalar multiplication because it scales, i.e., grows/shrinks vectors by a constant amount in all directions:\n",
    "\n",
    "![scaled vector](img/scaledvector.gif)\n",
    "\n",
    "Scalar multiplication is also super easy when you have a number and a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9. 2. 4. 8. 1.]\n",
      "3.0\n",
      "[27.  6. 12. 24.  3.]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "\n",
    "c = 3.\n",
    "print(c)\n",
    "print(c * v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pointwise multiplication is what numpy does with vectors by default__\n",
    "\n",
    "Here, \"pointwise\" once again means that the first elements are multiplied together, second elements are multiplied together, etc., to produce another vector of the same length. So, pointwise multiplication between $u = [u_1,u_2, \\cdots,u_n]$ and $v = [v_1,v_2, \\cdots,v_n]$ means the following:\n",
    "$$uv = [u_1v_1, u_2v_2, \\cdots, u_nv_n]$$\n",
    "\n",
    "While pointwise products are super useful to us for data handling and come up quite alot, they are not, on their own, particularly intuitively visualizable, and for this mathematical branch, are most often used as a step taken towards a different kind of product that we'll discuss next. However, pointwise products are once again quite easy and useful with numpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   3.7  2.  10.   0. ]\n",
      "[9. 2. 4. 8. 1.]\n",
      "[-45.    7.4   8.   80.    0. ]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "print(u)\n",
    "\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "print(u * v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__An inner product is just the sum of a pointwise product__\n",
    "\n",
    "While inner (a.k.a \"dot\") products build off of the pointwise product, they tell us about a lot more. Specifically, when two non-zero vectors have an inner product of $0$, they are orthogonal. Orthogonality is the fancy-math word for perpindicular when you're dealing with more than two dimensions. So, an inner product between $u = [u_1,u_2, \\cdots,u_n]$ and $v = [v_1,v_2, \\cdots,v_n]$ results in the scalar value—not a vector—produced by the following fomula:\n",
    "$$u\\cdot v = u_1v_1+u_2v_2+\\cdots+u_nv_n$$\n",
    "\n",
    "Intuitively, an inner product helps describe the projected length of one vector onto another. In particular, naming $u_v$ and $v_u$ as the projections of $v$ onto $u$ and $u$ onto $v$, respectively, we have:\n",
    "$$u\\cdot v = u_v\\|v\\| = v_u\\|u\\|$$\n",
    "\n",
    "So, when two vectors are a right-angle off of one another, their projections have length zero. Here's a graphical depiction of this intuitive description of an inner product:\n",
    "\n",
    "![inner](img/projection.gif)\n",
    "\n",
    "Now, if we want to find the inner product between two vectors, all we have to do is add up their pointwise product:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   3.7  2.  10.   0. ]\n",
      "[9. 2. 4. 8. 1.]\n",
      "50.4\n"
     ]
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "print(u)\n",
    "\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "print(sum(u*v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__However, the numpy way to do this uses the `.dot()` method on an array.__\n",
    "\n",
    "Recall: the inner produce is also called the dot product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   3.7  2.  10.   0. ]\n",
      "[9. 2. 4. 8. 1.]\n",
      "50.4\n"
     ]
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "print(u)\n",
    "\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "print(u.dot(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__The inner product makes it really easy to define the cosine similarity, and hence correlation measures.__\n",
    "\n",
    "Recall from last week that we spent a great deal of time building up from distance measures, etc., to finally arrive at a balanced data-comparison method called the correlation. Along the way, we made a stop at a similarity measure called cosine similarity. This took a bit of effort to express, but now with our linear algebra formalism we can write is succinctly:\n",
    "$$sim(u,v) = \\frac{u\\cdot v}{\\|u\\|\\|v\\|}$$\n",
    "\n",
    "and compute it quickly, using the fancy numpy method `.dot()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.   3.7  2.  10.   0. ]\n",
      "[9. 2. 4. 8. 1.]\n",
      "0.32747618583046045\n"
     ]
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "print(u)\n",
    "\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "print(v)\n",
    "\n",
    "sim = u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "print(sim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrices\n",
    "\n",
    "If it helps to think about vectors as the rows and columns of a spreadsheet, then think of the whole spreadsheet as a matrix—a collection of vectors (i.e., rows or columns) of the same length. More generally, a matrix is a two-dimensional ordered-array of numbers:\n",
    "\n",
    " $$ A = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\dots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\dots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} & a_{m,2} & \\dots & a_{m,n} \\end{bmatrix} $$\n",
    "\n",
    "\n",
    "While we could take a list of equal-length lists of numbers as a basic way to represet matrices in Python, numpy has already built in some very nice object types for working with matrices and performing matrix operations. Since there are now two dimensions of data inside of a a matrix it helps to get a bit of notation straight: when describing the numbers of rows and columns in a matrix it is conventional to list these numbers as row-by-column. So, an $m \\times n$ matrix, $A$, has $m$ rows and $n$ columns. Likewise, when indexing to indicate the individual elements of an $A$, it is customary to indicate the row position first, followed by the column position. So, the element $a_{i,j}$ refers to the number in the $i^\\text{th}$ row and $j^\\text{th}$ column of $A$. We can define matrices easily in numpy by using `numpy.array()` on lists of same-length lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Numpy still uses python 0-indexing, but matrices take two indices to access individual elements.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A)\n",
    "\n",
    "## pull out the value in row 2, column 3\n",
    "print(A[1, 2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Taking the transpose switches rows and columns__\n",
    "\n",
    "It's a bit strange to talk about this for spreadsheets—we've discussed why it's better to keep independent records as rows in a spreadsheet. However, matrices are more general numeric objects, and sometimes is's helpful to trade rows for columns:\n",
    "\n",
    "$$ A^T = \\begin{bmatrix} a_{1,1} & a_{2,1} & \\dots & a_{m,1} \\\\ a_{1,2} & a_{2,2} & \\dots & a_{m,2} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{1,n} & a_{2,n} & \\dots & a_{m,n} \\end{bmatrix} $$\n",
    "\n",
    "Taking the transpose is a easy as running `np.transpose` on a matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  4  7 10]\n",
      " [ 2  5  8 11]\n",
      " [ 3  6  9 12]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "## take the matrix transpose\n",
    "print(np.transpose(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Arithmetic__\n",
    "\n",
    "Like vectors, matrices have their own arithmetic and it is largely an extension or generalization of what is done for vectors. Numpy likewise has lots of good built-in functionality for this.\n",
    "\n",
    "__Addition & Subtraction__\n",
    "\n",
    "For two same-dimension matrices, addition is once again pointwise, but now according to row-column, i.e., $i,j$-position:\n",
    "\n",
    "$$ A + B = \\begin{bmatrix} a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots & a_{1,n} + b_{1,n} \\\\ a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots & a_{2,n} + b_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} + b_{m,1} & a_{m,2} + b_{m,2} & \\dots & a_{m,n} + b_{m,n} \\end{bmatrix} $$\n",
    "\n",
    "As it turns out, this is precisely what numpy does by default when you add two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[11 12 13]\n",
      " [14 15 16]\n",
      " [17 18 19]\n",
      " [20 21 22]] \n",
      "\n",
      "[[12 14 16]\n",
      " [18 20 22]\n",
      " [24 26 28]\n",
      " [30 32 34]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define another 4-row by 3-column matrix\n",
    "B = np.array([\n",
    "    [ 11,  12,  13],\n",
    "    [ 14,  15,  16],\n",
    "    [ 17,  18,  19],\n",
    "    [ 20,  21,  22]\n",
    "])\n",
    "\n",
    "print(B, '\\n')\n",
    "\n",
    "## take the matrix sum of the two\n",
    "print(A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiplication\n",
    "\n",
    "As with vectors, multiplication comes in multiple different flavors, and just like matrices make things more complicated, their multiplication is more complicated, too. However, like with vectors we'll start with the simplest forms possible and build up to what we need.\n",
    "\n",
    "__Multiplication by a scalar just multiplies every element__\n",
    "\n",
    "So, if we take an $m \\times n$ matrix $A$ and multiple it by a scalar (constant), $c$, we just get a $c$-grown or -shrunk version of the same:\n",
    "$$ cA = \\begin{bmatrix} ca_{1,1} & ca_{1,2} & \\dots & ca_{1,n} \\\\ ca_{2,1} & ca_{2,2} & \\dots & ca_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ ca_{m,1} & ca_{m,2} & \\dots & ca_{m,n} \\end{bmatrix} $$\n",
    "\n",
    "This is likewise what numpy does with scalars and matrices by default:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[ 10  20  30]\n",
      " [ 40  50  60]\n",
      " [ 70  80  90]\n",
      " [100 110 120]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define a constant\n",
    "c = 10\n",
    "\n",
    "## take the matrix sum of the two\n",
    "print(c*A)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "__Matrix times vector equals another vector__\n",
    "\n",
    "To multiply a matrix times a vector there is one major stipulation: if $A$ is an $m \\times n$ matrix then you can only multiply $A$ times a vector, $v$, as $A\\cdot v$ if $v$ is an $n \\times 1$ column vector. The result is then an $m \\times 1$ vector, which is once again called the _inner product_. Why? Because this is a generalization of the inner product for vectors. It works as follows:\n",
    "\n",
    "$$ \\begin{align} A\\cdot v & = \\begin{bmatrix} a_{1,1} & a_{1,2} & \\dots & a_{1,n} \\\\ a_{2,1} & a_{2,2} & \\dots & a_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m,1} & a_{m,2} & \\dots & a_{m,n} \\end{bmatrix} \\cdot \\begin{bmatrix} v_{1} \\\\ v_{2} \\\\ \\vdots \\\\ v_{n} \\end{bmatrix}\\\\\\\\ & = \\begin{bmatrix} a_{1,1}v_{1} + a_{1,2}v_{2} + \\cdots + a_{1,n}v_{n} \\\\ a_{2,1}v_{1} + a_{2,2}v_{2} + \\cdots + a_{2,n}v_{n} \\\\ \\vdots \\\\ a_{m,1}v_{1} + a_{m,2}v_{2} + \\cdots + a_{m,n}v_{n} \\end{bmatrix} \\end{align} $$\n",
    "\n",
    "Taking the inner product of a matrix and a vector like this is once again as straight forward as using the `.dot()` array method, specifically in the order: `A.dot(v)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[ 3210]\n",
      " [ 6540]\n",
      " [ 9870]\n",
      " [13200]] \n",
      "\n",
      "[ 3210  6540  9870 13200]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define a (3 x 1) vector\n",
    "v = np.array([\n",
    "    [10],\n",
    "    [100],\n",
    "    [1000]\n",
    "])\n",
    "\n",
    "## take the inner product: Av\n",
    "print(A.dot(v), '\\n')\n",
    "\n",
    "## note: numpy is forgiving, and will allow you \n",
    "## to multiply our matrix by a (1 x 3) vector, too\n",
    "v = np.array([10, 100, 1000])\n",
    "\n",
    "## take the inner product: Av\n",
    "print(A.dot(v))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Pointwise matrix multiplication is straightforward, and requires same-dimension matrices__\n",
    "\n",
    "Just like other pointwise operations, this kind of multiplication is straightforward and can be performed just with the usual _*_, i.e., times operator in Python:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[11 12 13]\n",
      " [14 15 16]\n",
      " [17 18 19]\n",
      " [20 21 22]] \n",
      "\n",
      "[[ 11  24  39]\n",
      " [ 56  75  96]\n",
      " [119 144 171]\n",
      " [200 231 264]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define another 4-row by 3-column matrix\n",
    "B = np.array([\n",
    "    [ 11,  12,  13],\n",
    "    [ 14,  15,  16],\n",
    "    [ 17,  18,  19],\n",
    "    [ 20,  21,  22]\n",
    "])\n",
    "\n",
    "print(B, '\\n')\n",
    "\n",
    "## take the pointwise matrix product of the two\n",
    "print(A * B)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Matrix times matrix equals another matrix (and it ain't always pretty)__\n",
    "\n",
    "As it turns out, this is also a matrix-matrix generalization of the inner product. Each column vector on the right is multiplied the same way as above to produce its own ouput column vector of rows. Just like with matrix times vector, there's an inner-dimension compatibility rule: the inner product, $A\\cdot B$, of two matrices, $A$, an ($\\ell \\times m$) matrix and $B$, an ($m \\times n$) matrix, exists when they have the same inner dimension, i.e., if $A$ has $m$ columns then $B$ must have $m$ rows. As it turns out, this common inner dimension collapses, with the result being a matrix of dimensions provided by the outer dimensions of the input matrices: ($\\ell \\times n$). The nicest way to think of this is probably as dot products of the row vectors on the left with column vectors on the right:\n",
    "\n",
    "![matrix mult](img/matrix_mult.png)\n",
    "\n",
    "Looking at it this way, left-rows and right-columns triangulate the output values of the product. For brevity, we won't present a full formula here, and like usual we can just use the `.dot()` numpy method from one array onto another to execute this computationally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[11 12]\n",
      " [13 14]\n",
      " [15 16]] \n",
      "\n",
      "[[ 82  88]\n",
      " [199 214]\n",
      " [316 340]\n",
      " [433 466]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define a 3-row by 2-column matrix\n",
    "B = np.array([\n",
    "    [ 11,  12],\n",
    "    [ 13,  14],\n",
    "    [ 15,  16],\n",
    "])\n",
    "\n",
    "print(B, '\\n')\n",
    "\n",
    "## take the inner matrix product\n",
    "## which results in a 4 x 4 matrix\n",
    "print(A.dot(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvectors and Eigenvalues\n",
    "\n",
    "While there's a lot of depth in this discussion that we'll have to avoid, the topic of eigenvectors is essential when thinking of linear algebra and matrices as modeling tools. The important concept with eigenvectors that we want to discuss has to do with matrix-times vector multiplication. Recall: an $(m \\times n)$ matrix $A$ times an $(n \\times 1)$ vector $v$ results in another vector, having dimension $(m \\times 1)$. So what if $m = n$, i.e., the matrix $A$ is _square_? Well, then we could take the result and multiply it by $A$ again! This is one of the things that eigenvectors are all about:\n",
    "\n",
    "+ if $A$ is an $(n \\times n)$ matrix and $v$ is an eigenvector of $A$, then for some non-zero scalar (constant), $\\lambda$:\n",
    "\n",
    "     \n",
    "$$A\\cdot v = \\lambda v$$\n",
    "\n",
    "$\\lambda$ is called the eigenvector's _eigenvalue_. In other words, matrix-times-eigenvector returns a vector that points in the same exact direction as the original eigenvector. You can keep multiplying the result by $A$ and get back scalings, i.e., growing/shrinking of the same vector. Later on, when we get into our discussion of networks we'll get to see how Google used this eigenvector concept this to build their PageRank algorithm, which truly enabled them to take over the web search market.\n",
    "\n",
    "So, how do you find these magical eigenvectors? That's a bit more of a tricky problem, and as it turns out a matrix has $n$ eigenvectors (they may just not all be real). However, since we're not going to be worrying about doing linear algebra by hand, we won't get into it here. Instead, let's just look at what comes for free with numpy via `numpy.linalg.eig()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [4 5]] \n",
      "\n",
      "[-0.46410162  6.46410162] \n",
      "\n",
      "[[-0.80689822 -0.34372377]\n",
      " [ 0.59069049 -0.9390708 ]] \n",
      "\n",
      "[-0.80689822  0.59069049]\n"
     ]
    }
   ],
   "source": [
    "## define a 2-row by 2-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2],\n",
    "    [ 4,  5]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "e_vals, e_vecs = np.linalg.eig(A)\n",
    "\n",
    "## the list of eigenvalues\n",
    "print(e_vals, '\\n')\n",
    "\n",
    "## the columns are our eigenvectors\n",
    "print(e_vecs, '\\n')\n",
    "\n",
    "## multiplying A by a column and dividing by \n",
    "## its eigenvalue results in the exact same column/eigenvector\n",
    "print(A.dot(e_vecs[:, 0]) / e_vals[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Geometrically, eigenvectors tell you the directions along which your data spread out__\n",
    "\n",
    "This means we can use eigenvectors to tell us about the variation present in a spreadsheet of data, i.e., about how columns and rows of data covary. So, if each point in a data set is a row, represented by two variable columns, we would be able to use eigenvectors to show us something like:\n",
    "\n",
    "![eigen](img/eigenvectors.png)\n",
    "\n",
    "__But wait, I thought eigenvectors were only for square matrices!__\n",
    "\n",
    "That's right, there's no such things as eigenvectors and eigenvales for an $m \\gt 2$ row by $2$-column matrix. This is why we need to bring together our statistical discussion of variance into matrices now.\n",
    "\n",
    "#### The covariance matrix\n",
    "\n",
    "Recall our formula for the variance of a vector (column) $x = [x_1, x_2, \\cdots, x_n]$:\n",
    "$$ \\sigma_{x,x}^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})(x_i-\\overline{x}) $$\n",
    "\n",
    "Its _covariance_ with another vector, $y = [y_1, y_2, \\cdots, y_n]$ is simply\n",
    "$$ \\sigma_{x,y}^2 = \\frac{1}{n}\\sum_{i=1}^n(x_i-\\overline{x})(y_i-\\overline{y}) $$\n",
    "\n",
    "This quantity describes how much the two vectors vary together. The covariance matrix of the $n$ columns of an $(m \\times n)$ matrix $A$ then records the covariance of every pairwise comparison of columns of $A$. Since $\\sigma_{x,y}^2 = \\sigma_{y,x}^2$, i.e., a different order records the same covariance, these values are recorded on both sides of this _symmetric_ square matrix. So, writing $\\sigma^2_{i,j}$ to indicate the covariance between the $i^\\text{th}$ and $j^\\text{th}$ columns, the covariance matrix is:\n",
    "\n",
    " $$ \\Sigma_A = \\begin{bmatrix} \\sigma^2_{1,1} & \\sigma^2_{1,2} & \\dots & \\sigma^2_{1,n} \\\\ \\sigma^2_{2,1} & \\sigma^2_{2,2} & \\dots & \\sigma^2_{2,n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\sigma^2_{n,1} & \\sigma^2_{n,2} & \\dots & \\sigma^2_{n,n} \\end{bmatrix} $$\n",
    "\n",
    "Once again and fortunately for us, numpy's got the goods to compute covariance easily. However, this numpy function defaults in a bit of a funny way, treating rows as variables and columns as observations. Thus, we'll have to set `rowvar = False` to get the covariance of our column-variables, which is probably more common in spreadsheet-style data.\n",
    "\n",
    "Note: You should ask youself in the below example: why are all of the covariance values the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[15. 15. 15.]\n",
      " [15. 15. 15.]\n",
      " [15. 15. 15.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,    2,  3],\n",
    "    [ 4,    5,  6],\n",
    "    [ 7,    8,  9],\n",
    "    [ 10,  11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## compute the covariance matrix of the columns\n",
    "print(np.cov(A, rowvar = False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
