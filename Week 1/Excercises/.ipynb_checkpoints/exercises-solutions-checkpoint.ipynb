{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A90YxtQLgcU1"
   },
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br> Chapter 1: Processing numeric data\n",
    "\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_NQSeY53gcU5"
   },
   "source": [
    "#### 1.0.2.3 Exercise: Creating a matrix \n",
    "Using the above matrix-generation techniques, create a 2x2 identity matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JfX9NyiYgcU6",
    "outputId": "9b1b2e51-c992-4db1-afaf-ef41e5ff3115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0.]\n",
      " [0. 1.]]\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "identit2 = np.zeros((2,2))\n",
    "\n",
    "for i in range(len(identit2)):\n",
    "    identit2[i,i] = 1\n",
    "\n",
    "print(identit2)\n",
    "print(np.identity(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6M-cP3QgcU9"
   },
   "source": [
    "#### 1.0.2.5 Exercise: broadcast operations \n",
    "Square each element in the matrix `A` without using any loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8rRPw1qrgcU9",
    "outputId": "ddcb72fd-a3aa-4f9a-f317-eba4750b3779"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  4  9 16 25]\n",
      "[ 1  4  9 16 25]\n"
     ]
    }
   ],
   "source": [
    "A = [1, 2, 3, 4, 5]\n",
    "\n",
    "A = np.array(A)\n",
    "\n",
    "A_squares = A ** 2\n",
    "\n",
    "print(A_squares)\n",
    "\n",
    "A_squares = np.power(A, 2)\n",
    "\n",
    "print(A_squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_7NJ_0ugcU-"
   },
   "source": [
    "#### 1.1.2.2 Exercise: vector arithmetic \n",
    "Calculate `A` - `B` for the provided vectors, without using a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J8zBAwDYgcU_",
    "outputId": "b3c922b8-b551-4202-9d4c-bddc999f67d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4]\n",
      "[0, -1, -2, -3, -4]\n",
      "[0 2 4 6 8]\n"
     ]
    }
   ],
   "source": [
    "A = [i for i in range(5)]\n",
    "B = [-i for i in range(5)]\n",
    "\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "A = np.array(A)\n",
    "B = np.array(B)\n",
    "\n",
    "print(A - B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-DrlPZlGgcU_",
    "outputId": "a1fce8da-0c22-4149-9258-d87eb599b116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.954451150103322 5.477225575051661\n"
     ]
    }
   ],
   "source": [
    "## compute euclidean norm of A - B:\n",
    "print(np.power(sum(np.power(A - B, 2)), 0.5), np.power(sum(np.power(A, 2)), 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ed8NpKKcgcVA"
   },
   "source": [
    "#### 1.1.2.5 Exercise: scalar multiplication\n",
    "Divide each element in the above vector `v` by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-GdXISJMgcVB",
    "outputId": "a7bc7fda-4142-4372-949f-3da9347dbbc9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.25, 0.5 , 1.  , 2.  , 0.25])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "\n",
    "v / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IhSkj_h1gcVB"
   },
   "source": [
    "#### 1.1.2.7 Exercise: pointwise vector multiplication \n",
    "Perform pointwise multiplication between `v` divided by 4 which you calculated above, and `u`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Yvv0KqR6gcVC",
    "outputId": "1f8cd71e-9d9d-49cf-bb39-5a64897f9453"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-11.25,   1.85,   2.  ,  20.  ,   0.  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "\n",
    "(v / 4) * u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TDhlsFpgcVD"
   },
   "source": [
    "#### 1.1.2.9 Exercise: inner products \n",
    "Find the dot product of the provided array `z` with `u` and `v`, respectively, from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DxWXCwLmgcVD",
    "outputId": "4cf01358-ed30-42ce-d728-bbb3bd3f7296"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.4\n",
      "62.0\n"
     ]
    }
   ],
   "source": [
    "z = np.array([1, 2, 3, 4, 5])\n",
    "\n",
    "u = np.array([-5., 3.7, 2., 10., 0.])\n",
    "v = np.array([9., 2., 4., 8., 1.])\n",
    "\n",
    "print(z.dot(u))\n",
    "print(z.dot(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5yIc7hHgcVE"
   },
   "source": [
    "#### 1.1.2.11 Exercise: cosine similarity \n",
    "Find the cosine similarity of the vectors `a` and `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HZ57MBNGgcVE",
    "outputId": "c96c4294-c4e9-4d54-df4c-87739b92997b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1, 0, 0])\n",
    "b = np.array([0, 1, 0])\n",
    "\n",
    "a.dot(b) / (np.linalg.norm(a) * np.linalg.norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ef5nY3VVgcVF"
   },
   "source": [
    "#### 1.1.3.3 Exercise: Matrix indexing \n",
    "Print the value in the above matrix `A` located in the bottom-right entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Xd9TL1HXgcVF",
    "outputId": "2d6c376c-d828-4d10-95e6-6bc815bacb0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A[-1,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tUNV3JHKgcVF"
   },
   "source": [
    "#### 1.1.4.2 Exercise: matrix addition \n",
    "Using `A` and `B` from above, find the sum `A` + `B` + `A`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fXenmmxogcVF",
    "outputId": "67548612-dc74-4a4f-e109-73151b888906"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  5  6]\n",
      " [ 7  8  9]\n",
      " [10 11 12]] \n",
      "\n",
      "[[11 12 13]\n",
      " [14 15 16]\n",
      " [17 18 19]\n",
      " [20 21 22]] \n",
      "\n",
      "[[13 16 19]\n",
      " [22 25 28]\n",
      " [31 34 37]\n",
      " [40 43 46]] \n",
      "\n",
      " [[13 16 19]\n",
      " [22 25 28]\n",
      " [31 34 37]\n",
      " [40 43 46]]\n"
     ]
    }
   ],
   "source": [
    "## define a 4-row by 3-column matrix\n",
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A, '\\n')\n",
    "\n",
    "## define another 4-row by 3-column matrix\n",
    "B = np.array([\n",
    "    [ 11,  12,  13],\n",
    "    [ 14,  15,  16],\n",
    "    [ 17,  18,  19],\n",
    "    [ 20,  21,  22]\n",
    "])\n",
    "\n",
    "print(B, '\\n')\n",
    "\n",
    "## take the matrix sum of the two\n",
    "print(2*A + B, \"\\n\\n\", A + A + B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vD2tOWKgcVG"
   },
   "source": [
    "#### 1.1.4.4 Exercise: Scalar matrix multiplication\n",
    "Divide each element in the above matrix `A` by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "J-CQ2FeKgcVG",
    "outputId": "4da7c60c-3ea0-4357-a030-0f729d830fce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "[[0.25 0.5  0.75]\n",
      " [1.   1.25 1.5 ]\n",
      " [1.75 2.   2.25]\n",
      " [2.5  2.75 3.  ]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([\n",
    "    [ 1,  2,  3],\n",
    "    [ 4,  5,  6],\n",
    "    [ 7,  8,  9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "print(A.shape)\n",
    "print(A/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-1p47ipglpg"
   },
   "source": [
    "## Additional In-depth Exercises\n",
    "\n",
    "### A. Getting to know numpy and word vectors\n",
    "While we won't begin with text analysis formally until next chapter, we can take some pre-computed data to start exploring language with linear algebraic methods.\n",
    "\n",
    "#### Main data\n",
    "The numpy array file in `path = './01-numeric/data/vectors-' + str(dim) + '-.npy'`, which is a `dim = 10` dimensional semantic representation for the words of Mary Shelly's Frankenstein, Or the Modern Prometheus. The object has three linear-algebraic components: $U$, $V$, and $b$ and each has a row-dimension $N$ that represents the vocabulary size. Once loaded these are layed out as:\n",
    "\n",
    "$$  \n",
    "\\begin{bmatrix} \n",
    "v_{1,1} & v_{1,2} & \\dots & v_{1,10} \\\\ \n",
    "v_{2,1} & v_{2,2} & \\dots & v_{2,10} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "v_{N,1} & v_{N,2} & \\dots & v_{N,10} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    "u_{1,1} & u_{1,2} & \\dots & u_{1,10} \\\\ \n",
    "u_{2,1} & u_{2,2} & \\dots & u_{2,10} \\\\ \n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\ \n",
    "u_{N,1} & u_{N,2} & \\dots & u_{N,10} \n",
    "\\end{bmatrix} \n",
    "\\begin{bmatrix} \n",
    " b_{1} \\\\ \n",
    " b_{2} \\\\ \n",
    " \\vdots \\\\ \n",
    " b_{N} \n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "#### A.1 Load a `numpy` matrix with pre-set values from disk.\n",
    "\n",
    "Load the vectors from disk using `np.load(path)` and slice the resulting object to produce `U`, `V`, and `b` arrays. Report the dimensions of each array to confirm their structure (and the size of the vocabulary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7273, 21), (7273, 10), (7273, 10), (7273,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "dim = 10\n",
    "vectors = np.load('./data/vectors-10.npy')\n",
    "V = vectors[:,:dim]\n",
    "U = vectors[:,dim:2*dim]\n",
    "b = vectors[:,-1]\n",
    "\n",
    "vectors.shape, V.shape, U.shape, b.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2 Save the `numpy` matrices with pre-set values from disk.\n",
    "Then, save these to disk under the paths:\n",
    "\n",
    "- `./01-numeric/data/U.npy`, \n",
    "- `./01-numeric/data/V.npy`, and \n",
    "- `./01-numeric/data/b.npy`.\n",
    "\n",
    "using the `np.save(path, array)` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"./data/V.npy\", V)\n",
    "np.save(\"./data/U.npy\", U)\n",
    "np.save(\"./data/b.npy\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.3 Load the linked data for the vectors and build the word index\n",
    "Load `data = json.load(open('./data/vectors-linked_data.json'))` and inspect the resulting object. For now, you'll only have to utilize the dictionary value within it, called `data['counts']`, which is a dictonary, keyed by words, with count-occurrence values (frequencies).\n",
    "\n",
    "Using `data['counts']`, construct a `word_index` object, which is a dictionary of the form:\n",
    "```\n",
    "word_index = {\n",
    "    w: representation_index,\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "such that `w` is any word string keying `data['counts']` and `representation_index` is the row index for `w` `U`-`V`-`b` representation of `w`. Hint: Python (>3) dictionaries hold keys in the order in which they were originally loaded, i.e., presentated in the json string serialization format. Use this and the fact that the the words were counted in the book's reading order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['counts', 'm_map', 'position', 'sentences'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "data = json.load(open('./data/vectors-linked_data.json'))\n",
    "print(data.keys())\n",
    "word_index = {w: i for i, w in enumerate(data['counts'])}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.4 Write a flexible concatenation function\n",
    "We'll want to conditionally concatenate the different representation components `U`, `V`, and `b`, but will assume that we're always going to use `U`. So write a function called `concatenate(U, V = 0, b = 0)` and stacks as many are non-zero size-by-side (as columns).\n",
    "\n",
    "[Hint. Use the `np.column_stack()` function!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7273, 21)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def concatenate(U, V = 0, b = 0):\n",
    "    to_concatenate = []\n",
    "    for thing in [U, V, b]: \n",
    "        if type(thing) != int: to_concatenate.append(thing)\n",
    "    if not to_concatenate: \n",
    "        print(\"at least one array must be non-empty\")\n",
    "        return np.array([])\n",
    "    else:\n",
    "        return(np.column_stack(tuple(to_concatenate)))\n",
    "concatenate(U, V, b).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.5 Write a cosine similarity functions that determines the most similar (word) vectors\n",
    "Use the `concatenate()` function from the previous cell to flexibly stack the different columns. This will allow us to explore how/where the representations stores semantics.\n",
    "\n",
    "_Accepts_:\n",
    "- `w`: the target string to measure similarity against\n",
    "- `U`, `V (= 0)`, `b (= 0)`: the semantic arrays from the representation or integer (null)\n",
    "- `top (= 0)`: an integer describing the number of 'most similar' words/scores, and\n",
    "- `v (= 0)`: a `V` (plus `U` and/or `b` dimensional) vector to which to compare to the vectors, instead of the target word.\n",
    "\n",
    "_Returns_:\n",
    "- `w_sims`: a sorted list of `top` tuples, of form: `(v, w_v_similarity)`, sortet high to low by `w_v_similarity` (cosine) values between vectors for words `w` and `v`.\n",
    "\n",
    "Note: vectors must be unit-normed in order to compute these similarities!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she', 1.0),\n",
       " ('her', 0.9738238945046376),\n",
       " ('very', 0.9727472319478015),\n",
       " ('we', 0.9719620112096093),\n",
       " ('he', 0.9655729080775022),\n",
       " ('justine', 0.964113201603028),\n",
       " ('dear', 0.9623722968823758),\n",
       " ('his', 0.961025308751945),\n",
       " ('poor', 0.949050515881002),\n",
       " ('a', 0.9445971362834037)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def most_similar(w, word_index, U, V=0, b=0, top=10, v = 0):\n",
    "    vec = concatenate(U, V, b)\n",
    "    vec = vec / np.linalg.norm(vec, axis=1)[:, np.newaxis] # broadcasting\n",
    "    if type(v) == int: v = vec[word_index[w],:]\n",
    "    similar = sorted(enumerate(list(vec.dot(v))), \n",
    "                          key = lambda x: x[1], reverse = True)\n",
    "    types = list(word_index.keys())\n",
    "    if not top: top = len(vec.shape[0])\n",
    "    word_sims = [(types[ix], sim) for ix, sim in similar[:top]]\n",
    "    return word_sims\n",
    "most_similar('she', word_index, U, V, b, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.6 Build an analogy generator\n",
    "The classic motivation for the utility of word2vec is it's capacity to represent semantic constructs that 'locally' obey liear relationships. The most well known semantic constructs represented are loosely analogies, e.g., across gender, like:\n",
    "$$\n",
    "\\hat{v}_\\text{queen}\\approx \\frac{1}{3}\\left(v_\\text{king} - v_\\text{man} + v_\\text{woman}\\right)\n",
    "$$.\n",
    "\n",
    "Here, build an analogy generator by writing a function:\n",
    "- `analogy(positive, negative, word_index, U, V=0, b=0, top=10)`,\n",
    "which accepts a list of two words called `positive` and a string called `negative` (in addition to the others from __A.4__), computes the above, and (uses this as `v` in the result of __A.4__) to compute the most similar other words in the vocabulary to the specified linear combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greatest', 0.1826567889499377),\n",
       " ('mother', 0.16515019232053824),\n",
       " ('of', 0.13401675657011436),\n",
       " ('m', 0.12988934133664037),\n",
       " ('to', 0.12635711683792133),\n",
       " ('child', 0.12585030505433098),\n",
       " ('years', 0.1234544843702866),\n",
       " ('time', 0.12307770986396062),\n",
       " ('and', 0.1207347591632278),\n",
       " ('very', 0.11414261424913746),\n",
       " ('natural', 0.11145783399374459),\n",
       " ('in', 0.11083148324067152),\n",
       " ('had', 0.10471441583413287),\n",
       " ('which', 0.10229660669490491),\n",
       " ('was', 0.09951253650731141),\n",
       " ('a', 0.0986087505470151),\n",
       " ('science', 0.09816803333510984),\n",
       " ('her', 0.0976394831690304),\n",
       " ('i', 0.0953172936524621),\n",
       " ('most', 0.09388143215089532),\n",
       " ('waldman', 0.08835233157641761),\n",
       " ('agrippa', 0.08767361614651456),\n",
       " ('dear', 0.08749062401763066),\n",
       " ('on', 0.08683379748941522),\n",
       " ('been', 0.08601971701558453)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def analogy(positive, negative, word_index, U, V=0, b=0, top=10):\n",
    "    vec = concatenate(U, V, b)\n",
    "    v_hat = (vec[[word_index[w] for w in positive],:].sum(axis = 0) - vec[word_index[negative],:])/3\n",
    "    return most_similar('', word_index, U, V, b, top, v_hat)\n",
    "analogy(['father', 'creation'], 'monster', word_index, U, V, b, 25)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercises.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
