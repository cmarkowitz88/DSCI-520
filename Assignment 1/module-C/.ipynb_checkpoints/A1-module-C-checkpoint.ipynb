{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module C _(35 points)_ Similarity of word usage across a document\n",
    "\n",
    "Here we'll be building up some code to discover how different terms are utilized similarly across a document. For this, our first task will be to create a word frequency counting function.\n",
    "\n",
    "__C1.__ _(12 points)_ Define a function called `count_words(paragraph, pos = True, lemma = True)` that `return`s a `Counter()` called `frequency`. In `frequency`, each key will consist of a `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`. The `Counter()` should simply contain the number of times each `heading` is observed in the `paragraph`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C1:Function(12/12)\n",
    "\n",
    "from collections import Counter\n",
    "import spacy, json, re\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def count_words(paragraph, pos = True, lemma = True):\n",
    "\n",
    "    frequency = Counter()\n",
    "    doc = nlp(paragraph)\n",
    "    # print(doc)\n",
    "\n",
    "    for word in doc:\n",
    "        # print(word)\n",
    "\n",
    "        text = word.text\n",
    "        tag = ''\n",
    "\n",
    "        if lemma:\n",
    "            text = word.lemma_\n",
    "\n",
    "        if pos:\n",
    "            tag = word.pos_\n",
    "\n",
    "        heading = (text, tag)\n",
    "\n",
    "        frequency[heading] += 1\n",
    "    \n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure your function works by testing it on a short sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown fox jumps over the lazy dog.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({('the', 'DET'): 2,\n",
       "         ('quick', 'ADJ'): 1,\n",
       "         ('brown', 'ADJ'): 1,\n",
       "         ('fox', 'PROPN'): 1,\n",
       "         ('jump', 'VERB'): 1,\n",
       "         ('over', 'ADP'): 1,\n",
       "         ('lazy', 'ADJ'): 1,\n",
       "         ('dog', 'NOUN'): 1,\n",
       "         ('.', 'PUNCT'): 1})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C1:SanityCheck\n",
    "\n",
    "count_words(\"The quick brown fox jumps over the lazy dog.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C2.__ _(8 pts)_ Next, define a function called `book_TDM(book_id, pos = True, lemma = True)` and copy into it the TDM-producing code from __Section 2.1.5.1__ of the lecture notes, now `return`-ing `TDM` and `all_words`. Once copied, modify this function to call `count_words` appropriately, now passing through the user of `book_TDM`'s specified `lemma` and `pos` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C2:Function(8/8)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def load_book(book_id):\n",
    "    paragraphs = []\n",
    "\n",
    "    try:\n",
    "        with open(\"./data/books/\" + book_id + \".txt\") as file:\n",
    "            book_data = file.read()\n",
    "            trimmed_book_data = book_data.strip()\n",
    "            paragraphs = re.split('\\n{2,}', trimmed_book_data)\n",
    "            # print(trimmed_book_data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error, file not found.\")\n",
    "        sys.exit()\n",
    "\n",
    "    return paragraphs\n",
    "\n",
    "\n",
    "def book_TDM(book_id, pos = True, lemma = True):\n",
    "\n",
    "    paragraphs = load_book('84')\n",
    "\n",
    "    # the 'master' set, keeps track of the words in all documents\n",
    "    all_words = set()\n",
    "\n",
    "    # store the word frequencies by book\n",
    "    all_doc_frequencies = {}\n",
    "    j = 0\n",
    "    for text in paragraphs[:10]:\n",
    "        doc = nlp(text)\n",
    "\n",
    "        # loop over the sentences\n",
    "        # for j, sentence in enumerate(doc.sents):\n",
    "\n",
    "        # CM NOTE: Using the index j it will get reset to 0 each time we read a new paragraph... Is that we really want\n",
    "        # Created another j that never gets reset\n",
    "        for sentence in doc.sents:\n",
    "            frequency = count_words(sentence.text)\n",
    "            all_doc_frequencies[j] = frequency\n",
    "            doc_words = set(frequency.keys())\n",
    "            all_words = all_words.union(doc_words)\n",
    "            j += 1\n",
    "\n",
    "    # create a matrix of zeros: (words) x (documents)\n",
    "    TDM = np.zeros((len(all_words), len(all_doc_frequencies)))\n",
    "\n",
    "    # fix a word ordering for the rows\n",
    "    all_words = sorted(list(all_words))\n",
    "\n",
    "    # loop over the (sorted) document numbers and (ordered) words; fill in matrix\n",
    "    for j in all_doc_frequencies:\n",
    "        for i, word in enumerate(all_words):\n",
    "            TDM[i, j] = all_doc_frequencies[j][word]\n",
    "            \n",
    "    return TDM, all_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To test your code's function, let's process `book_id = 84` with both of `pos = True` and `lemma = True` and print out the `TDM`'s `.shape` attribute and the first ten terms in `all_words`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\n', 'SPACE'),\n",
       " (' ', 'SPACE'),\n",
       " ('(', 'PUNCT'),\n",
       " (')', 'PUNCT'),\n",
       " (',', 'PUNCT'),\n",
       " ('-', 'PUNCT'),\n",
       " ('--', 'PUNCT'),\n",
       " ('-PRON-', 'DET'),\n",
       " ('-PRON-', 'PRON'),\n",
       " ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "TDM, terms = book_TDM(\"84\", pos = True, lemma = True)\n",
    "terms[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6262, 40)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# C2:SanityCheck\n",
    "\n",
    "TDM.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C3.__ _(8 pts)_ Next, your job is to define two functions. The first is `sim(u,v)`, which shoud take two arbitrary numeric vectors and compute/output the `cosine_similarity`, as described in __Section 1.1.2.10__.  \n",
    "\n",
    "The second function is `term_sims(i, TDM)`, which should utilize the first function (`sim` function) to output a list of cosine similarity values (`sim_values`) between the word/row `i` and all others (rows) in the `TDM`.\n",
    "\n",
    "Note: each of these functions can be straightforwardly completed using a single line of code! Exhibit your knowledge of comprehensions and vectorization!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3:Function(4/8)\n",
    "def sim(u,v):\n",
    "    \n",
    "    cosine_similarity = u.dot(v) / (np.linalg.norm(u) * np.linalg.norm(v))\n",
    "    \n",
    "    return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exactly similar: 1.0\n",
      "Exactly dissimilar: -1.0\n",
      "In the middle: 0.0\n"
     ]
    }
   ],
   "source": [
    "# C3:SanityCheck\n",
    "\n",
    "print(\"Exactly similar:\", sim(np.array([1,2,3]), np.array([1,2,3])))\n",
    "print(\"Exactly dissimilar:\", sim(np.array([1,2,3]), np.array([-1,-2,-3])))\n",
    "print(\"In the middle:\", sim(np.array([1,1]), np.array([-1,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C3:Function(4/8)\n",
    "\n",
    "def term_sims(i, TDM):\n",
    "    \n",
    "    sim_values = [sim(TDM[i], TDM[idx]) for idx, itm in enumerate(TDM)]\n",
    "    \n",
    "    return sim_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0000000000000002, 0.558156305651438, 0.0, 0.0, 0.8097316102668098, 0.0, 0.3922322702763681, 0.6908817223234606, 0.8425608199604728, 0.8608284620211167, 0.0, 0.0, 0.0, 0.6227523687795278, 0.06933752452815364, 0.0, 0.09805806756909202, 0.09805806756909202, 0.09805806756909202, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09805806756909202]\n"
     ]
    }
   ],
   "source": [
    "# C3:SanityCheck\n",
    "\n",
    "# Compare word/row 0 to all other (rows) in the TDM\n",
    "out_list = term_sims(0, TDM)\n",
    "print(out_list[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__C4.__ _(7 pts)_ Finally, your goal now is to a write function, `most_similar(term, terms, TDM, top = 25)`, that utilizes `term_sims` to output a sorted list of the `top = N` terms (`top_n_terms`) most similar to one specified (`term`). The output data type should be a list of lists, with each inner list representing information for a similar term as: `[row_ix, similarity, term]`. \n",
    "\n",
    "\\[Hint: to locate the row containing the term of interest, utilize the list `.index()` method in application to the `terms` argument.\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:Function(6/7)\n",
    "\n",
    "def most_similar(term, terms, TDM, top = 25):\n",
    "    \n",
    "    #---Your code starts here---\n",
    "\n",
    "    #---Your code ends here---\n",
    "    \n",
    "    return top_n_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your functions utility on a `TDM` produced for `book_id = 84` and exhibit the top 25 similar terms to both of `('monster', 'NOUN')` and `('beautiful', 'ADJ')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:SanityCheck\n",
    "\n",
    "most_similar(('monster', 'NOUN'), terms, TDM, top = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:SanityCheck\n",
    "\n",
    "most_similar(('beautiful', 'ADJ'), terms, TDM, top = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C4:Inline\n",
    "\n",
    "# Comment on the ordered results returned in the sanity checks.\n",
    "# Do you think the algorithm is exhibiting sensible results? print \"Yes\" or \"No\"\n",
    "print(\"<ANSWER>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
