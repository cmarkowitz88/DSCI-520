{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment group 1: Textual feature extraction and numerical comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module B _(35 points)_ Key word in context\n",
    "\n",
    "Key word in context (KWiC) is a common format for concordance lines, i.e., contextualized instances of principal words used in a book. More generally, KWiC is essentially the concept behind the utility of 'find in page' on document viewers and web browsers. This module builds up a KWiC utility for finding key word-containing sentences, and 'most relevant' paragraphs, quickly.\n",
    "\n",
    "__B1.__ _(3 points)_ Start by writing a function called `load_book` that reads in a book based on a provided `book_id` string and returns a list of `paragraphs` from the book. When book data is loaded, you should remove the space characters at the beginning and end of the text (e.g., using `strip()`). Then, to split books into paragraphs, use the `re.split()` method to split the input in cases where there are two or more new lines. Note, that books are in the provided `data/books` directory.\n",
    "\n",
    "Note: this module is not focused on text pre-processing beyond a split into paragraphs; you do _not_ need to remove markup or non-substantive content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B1:Function(3/3)\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def load_book(book_id):\n",
    "    \n",
    "    paragraphs = []\n",
    "\n",
    "    try:\n",
    "        with open(\"./data/books/\" + book_id + \".txt\") as file:\n",
    "            book_data = file.read()\n",
    "            trimmed_book_data = book_data.strip()\n",
    "            paragraphs = re.split('\\n{2,}', trimmed_book_data)\n",
    "            # print(trimmed_book_data)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error, file not found.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    return paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your function, lets apply it to look at a few paragraphs from book 84."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "723\n",
      "These reflections have dispelled the agitation with which I began my\n",
      "letter, and I feel my heart glow with an enthusiasm which elevates me\n",
      "to heaven, for nothing contributes so much to tranquillize the mind as\n",
      "a steady purpose--a point on which the soul may fix its intellectual\n",
      "eye.  This expedition has been the favourite dream of my early years. I\n",
      "have read with ardour the accounts of the various voyages which have\n",
      "been made in the prospect of arriving at the North Pacific Ocean\n",
      "through the seas which surround the pole.  You may remember that a\n",
      "history of all the voyages made for purposes of discovery composed the\n",
      "whole of our good Uncle Thomas' library.  My education was neglected,\n",
      "yet I was passionately fond of reading.  These volumes were my study\n",
      "day and night, and my familiarity with them increased that regret which\n",
      "I had felt, as a child, on learning that my father's dying injunction\n",
      "had forbidden my uncle to allow me to embark in a seafaring life.\n"
     ]
    }
   ],
   "source": [
    "# B1:SanityCheck\n",
    "paragraphs = load_book('84')\n",
    "print(len(paragraphs))\n",
    "print(paragraphs[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B2.__ _(10 points)_ Next, write a function called `kwic(paragraphs, search_terms)` that accepts a list of string `paragraphs` and a set of `search_term` strings. The function should:\n",
    "\n",
    "1. initialize `data` as a `defaultdict` of lists\n",
    "2. loop over the `paragraphs` and apply `spacy`'s processing to produce a `doc` for each;\n",
    "3. loop over the `doc.sents` resulting from each `paragraph`;\n",
    "4. loop over the words in each `sentence`;\n",
    "5. check: `if` a `word` is `in` the `search_terms` set;\n",
    "6. `if` (5), then `.append()` the reference under `data[word]` as a list: `[[i, j, k], sentence]`, where `i`, `j`, and `k` refer to the paragraph-in-book, sentence-in-paragraph, and word-in-sentence indices, respectively.\n",
    "\n",
    "Your output, `data`, should then be a default dictionary of lists of the format:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```\n",
    "\n",
    "Note, we have imported spacy and set it up to use the `\"en\"` model. This will require you to install spacy by running `pip install spacy` and downloading the `\"en\"` model by running the command `python -m spacy download en`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B2:Function(10/10)\n",
    "\n",
    "from collections import defaultdict\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def def_value():\n",
    "    tmp_list = []\n",
    "    return tmp_list\n",
    "\n",
    "def kwic(paragraphs, search_terms = {}):\n",
    "    \n",
    "    data = defaultdict(def_value)\n",
    "    idx_paragraph = 0\n",
    "    idx_sentence = 0\n",
    "\n",
    "    for para in paragraphs:\n",
    "        idx_paragraph += 1\n",
    "        doc = nlp(para)\n",
    "\n",
    "        for sent in doc.sents:\n",
    "            idx_sentence += 1\n",
    "            # print(sent)\n",
    "\n",
    "            word_list = []\n",
    "            for token in sent:\n",
    "                word_list.append(token.text)\n",
    "\n",
    "            for token in sent:\n",
    "                # print(token)\n",
    "\n",
    "                if token.text in search_terms:\n",
    "                    # print('found it')\n",
    "                    tmp_list = [[idx_paragraph, idx_sentence, token.idx], word_list]\n",
    "                    data[token.text].append(tmp_list)\n",
    "\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your function using the paragraphs from your `load_book` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.def_value()>,\n",
       "            {'Ocean': [[[11, 28, 479],\n",
       "               ['I',\n",
       "                '\\n',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                '\\n',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole',\n",
       "                '.',\n",
       "                ' ']]],\n",
       "             'seas': [[[11, 28, 497],\n",
       "               ['I',\n",
       "                '\\n',\n",
       "                'have',\n",
       "                'read',\n",
       "                'with',\n",
       "                'ardour',\n",
       "                'the',\n",
       "                'accounts',\n",
       "                'of',\n",
       "                'the',\n",
       "                'various',\n",
       "                'voyages',\n",
       "                'which',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'been',\n",
       "                'made',\n",
       "                'in',\n",
       "                'the',\n",
       "                'prospect',\n",
       "                'of',\n",
       "                'arriving',\n",
       "                'at',\n",
       "                'the',\n",
       "                'North',\n",
       "                'Pacific',\n",
       "                'Ocean',\n",
       "                '\\n',\n",
       "                'through',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'which',\n",
       "                'surround',\n",
       "                'the',\n",
       "                'pole',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[22, 121, 1164],\n",
       "               ['Shall',\n",
       "                'I',\n",
       "                'meet',\n",
       "                'you',\n",
       "                'again',\n",
       "                ',',\n",
       "                'after',\n",
       "                'having',\n",
       "                'traversed',\n",
       "                'immense',\n",
       "                'seas',\n",
       "                ',',\n",
       "                'and',\n",
       "                '\\n',\n",
       "                'returned',\n",
       "                'by',\n",
       "                'the',\n",
       "                'most',\n",
       "                'southern',\n",
       "                'cape',\n",
       "                'of',\n",
       "                'Africa',\n",
       "                'or',\n",
       "                'America',\n",
       "                '?',\n",
       "                ' ']],\n",
       "              [[31, 146, 117],\n",
       "               ['Thus',\n",
       "                'far',\n",
       "                'I',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'gone',\n",
       "                ',',\n",
       "                'tracing',\n",
       "                'a',\n",
       "                'secure',\n",
       "                'way',\n",
       "                'over',\n",
       "                'the',\n",
       "                'pathless',\n",
       "                'seas',\n",
       "                ',',\n",
       "                'the',\n",
       "                'very',\n",
       "                'stars',\n",
       "                '\\n']],\n",
       "              [[379, 2082, 566],\n",
       "               ['I',\n",
       "                'had',\n",
       "                'a',\n",
       "                'very',\n",
       "                '\\n',\n",
       "                'confused',\n",
       "                'knowledge',\n",
       "                'of',\n",
       "                'kingdoms',\n",
       "                ',',\n",
       "                'wide',\n",
       "                'extents',\n",
       "                'of',\n",
       "                'country',\n",
       "                ',',\n",
       "                'mighty',\n",
       "                'rivers',\n",
       "                ',',\n",
       "                '\\n',\n",
       "                'and',\n",
       "                'boundless',\n",
       "                'seas',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[669, 3519, 1273],\n",
       "               ['I',\n",
       "                'had',\n",
       "                'determined',\n",
       "                ',',\n",
       "                'if',\n",
       "                'you',\n",
       "                'were',\n",
       "                'going',\n",
       "                'southwards',\n",
       "                ',',\n",
       "                'still',\n",
       "                'to',\n",
       "                'trust',\n",
       "                '\\n',\n",
       "                'myself',\n",
       "                'to',\n",
       "                'the',\n",
       "                'mercy',\n",
       "                'of',\n",
       "                'the',\n",
       "                'seas',\n",
       "                'rather',\n",
       "                'than',\n",
       "                'abandon',\n",
       "                'my',\n",
       "                'purpose',\n",
       "                '.',\n",
       "                ' ']],\n",
       "              [[677, 3584, 1652],\n",
       "               ['Behold',\n",
       "                ',',\n",
       "                'on',\n",
       "                'these',\n",
       "                'desert',\n",
       "                'seas',\n",
       "                'I',\n",
       "                'have',\n",
       "                '\\n',\n",
       "                'found',\n",
       "                'such',\n",
       "                'a',\n",
       "                'one',\n",
       "                ',',\n",
       "                'but',\n",
       "                'I',\n",
       "                'fear',\n",
       "                'I',\n",
       "                'have',\n",
       "                'gained',\n",
       "                'him',\n",
       "                'only',\n",
       "                'to',\n",
       "                'know',\n",
       "                'his',\n",
       "                'value',\n",
       "                '\\n',\n",
       "                'and',\n",
       "                'lose',\n",
       "                'him',\n",
       "                '.',\n",
       "                ' ']]]})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B2:SanityCheck\n",
    "kwic(paragraphs, {'Ocean', 'seas'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B3.__ _(2 points)_ Let's test your `kwic` search function's utility using the pre-processed `paragraphs` from book `84` for the key words `Frankenstein` and `monster` in context. Answer the inline questions about these tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "She nursed Madame \n",
      " Frankenstein , my aunt , in her last illness , with the greatest affection \n",
      " and care and afterwards attended her own mother during a tedious \n",
      " illness , in a manner that excited the admiration of all who knew her , \n",
      " after which she again lived in my uncle 's house , where she was beloved \n",
      " by all the family .  \n",
      "\n",
      "I started from my sleep with horror ; a cold dew covered my forehead , my \n",
      " teeth chattered , and every limb became convulsed ; when , by the dim and \n",
      " yellow light of the moon , as it forced its way through the window \n",
      " shutters , I beheld the wretch -- the miserable monster whom I had \n",
      " created .  \n"
     ]
    }
   ],
   "source": [
    "# B3:SanityCheck\n",
    "results = kwic(paragraphs, {\"Frankenstein\", \"monster\"})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SLOOOW\n"
     ]
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# Is the kwic function fast or slow? Print \"Fast\" or \"Slow\"\n",
    "print()\n",
    "print(\"SLOOOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B3:Inline(1/2)\n",
    "\n",
    "# How many sentences does the work Frankenstein appear in? Print the integer (0 is just a placeholder).\n",
    "# print(0)\n",
    "len(results['Frankenstein'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B4.__ _(10 pts)_ The cost of _indexing_ a given book turns out to be the limiting factor here for kwic. Presently, we have our pre-processing `load_book` function just splitting a document into paragraphs. Rewrite the `load_book` function to do some additional preprocessing. Specifically, this function should be modified to:\n",
    "\n",
    "1. split a `book` into paragraphs and loop over them, but\n",
    "2. process each paragraph with `spacy`;\n",
    "3. store the `document` as a triple-nested list, so that each word _string_ is reachable via three indices: `word = document[i][j][k]`;\n",
    "4. record an `index = defaultdict(list)` containing a list of `[i,j,k]` lists for each word; and\n",
    "5. `return document, index`\n",
    "\n",
    "Pre-computing the `index` will allow us to efficiently look up the locations of each word's instance in `document`, and the triple-list format of our document will allow us fast access to extract the sentence for KWiC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:Function(10/10)\n",
    "\n",
    "def load_book_2(book_id):\n",
    "    \n",
    "    idx_paragraph = 0\n",
    "    idx_sentence = 0\n",
    "    idx_word = 0\n",
    "\n",
    "    document = []\n",
    "    index = defaultdict(def_value)\n",
    "\n",
    "    '''\n",
    "    tst_lst = [['paragraph1', ['sentence1', 'sentence2']], ['paragraph2']]\n",
    "    print(tst_lst[0])\n",
    "    print(tst_lst[0][1])\n",
    "    l = ['a', 'b', ['cc', 'dd', ['eee', 'fff']], 'g', 'h']\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(\"./data/books/\" + book_id + \".txt\") as file:\n",
    "            book_data = file.read()\n",
    "            trimmed_book_data = book_data.strip()\n",
    "            paragraphs = re.split('\\n{2,}', trimmed_book_data)\n",
    "\n",
    "            for para in paragraphs:\n",
    "                para_list = []\n",
    "                doc = nlp(para)\n",
    "                para_list.append(doc.text)\n",
    "                document.append(para_list)\n",
    "\n",
    "                # sent_list = []\n",
    "                for sent in doc.sents:\n",
    "                    sent_list = []\n",
    "                    sent_list.append(sent.text)\n",
    "                    # doc_list[idx_paragraph].append(sent_list)\n",
    "\n",
    "                    word_list = []\n",
    "                    idx_word = 0\n",
    "\n",
    "                    for token in sent:\n",
    "                        word_list.append(token.text)\n",
    "\n",
    "                        indicies_list = [idx_paragraph, idx_sentence, idx_word]\n",
    "                        index[token.text].append(indicies_list)\n",
    "\n",
    "                        idx_word += 1\n",
    "\n",
    "                    sent_list.append(word_list)\n",
    "                    document[idx_paragraph].append(sent_list)\n",
    "\n",
    "                    idx_sentence += 1\n",
    "\n",
    "\n",
    "                idx_paragraph += 1\n",
    "                idx_sentence = 0\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error, file not found.\")\n",
    "        sys.exit()\n",
    "\n",
    "    return(document, index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test your new function on `book_id` = `'84'`. We'll use the returned document to access a particular sentence and print out the `[i,j,k]` locations of the word `'monster'` from `index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# load the book\n",
    "document, index = load_book_2(\"84\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the hue of death; her features appeared to change, and I thought that I\\nheld the corpse of my dead mother in my arms; a shroud enveloped her\\nform, and I saw the grave-worms crawling in the folds of the flannel.\\n',\n",
       " ['the',\n",
       "  'hue',\n",
       "  'of',\n",
       "  'death',\n",
       "  ';',\n",
       "  'her',\n",
       "  'features',\n",
       "  'appeared',\n",
       "  'to',\n",
       "  'change',\n",
       "  ',',\n",
       "  'and',\n",
       "  'I',\n",
       "  'thought',\n",
       "  'that',\n",
       "  'I',\n",
       "  '\\n',\n",
       "  'held',\n",
       "  'the',\n",
       "  'corpse',\n",
       "  'of',\n",
       "  'my',\n",
       "  'dead',\n",
       "  'mother',\n",
       "  'in',\n",
       "  'my',\n",
       "  'arms',\n",
       "  ';',\n",
       "  'a',\n",
       "  'shroud',\n",
       "  'enveloped',\n",
       "  'her',\n",
       "  '\\n',\n",
       "  'form',\n",
       "  ',',\n",
       "  'and',\n",
       "  'I',\n",
       "  'saw',\n",
       "  'the',\n",
       "  'grave',\n",
       "  '-',\n",
       "  'worms',\n",
       "  'crawling',\n",
       "  'in',\n",
       "  'the',\n",
       "  'folds',\n",
       "  'of',\n",
       "  'the',\n",
       "  'flannel',\n",
       "  '.',\n",
       "  '\\n']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# B4:SanityCHeck\n",
    "\n",
    "# Output paragraph 9, sentence 5\n",
    "document[124][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124, 10, 57], [136, 3, 6], [139, 3, 4], [142, 1, 4], [243, 3, 29], [261, 3, 18], [280, 0, 2], [321, 1, 35], [345, 9, 6], [380, 13, 5], [397, 1, 46], [437, 0, 16], [439, 0, 3], [477, 7, 8], [478, 7, 6], [510, 1, 8], [527, 0, 1], [538, 19, 22], [560, 3, 31], [585, 4, 43], [587, 11, 72], [606, 3, 2], [615, 2, 11], [633, 10, 9], [639, 1, 21], [644, 4, 17], [653, 8, 5], [663, 5, 2], [673, 0, 39], [673, 2, 2], [709, 11, 1]]\n",
      "\n",
      "\n",
      "[[1, 0, 0], [103, 3, 11], [131, 5, 4], [134, 2, 4], [165, 8, 15], [184, 0, 11], [187, 0, 3], [232, 6, 4], [253, 5, 0], [283, 8, 2], [285, 3, 3], [285, 21, 5], [439, 2, 10], [440, 0, 2], [485, 7, 5], [673, 4, 6], [673, 10, 0], [686, 2, 0], [688, 3, 8], [695, 4, 0], [706, 3, 25], [709, 2, 2], [710, 1, 28], [712, 1, 2], [713, 0, 21], [715, 0, 5], [720, 2, 2]]\n"
     ]
    }
   ],
   "source": [
    "# B4:SanityCheck\n",
    "\n",
    "# Output the indices for monster\n",
    "print(index['monster'])\n",
    "print()\n",
    "print()\n",
    "print(index['Frankenstein'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B5.__ _(5 pts)_ Finally, make a new function called `fast_kwic` that takes a `document` and `index` from our new `load_book` function as well as a provided list of `search_terms` (just like our original kwic function). The function should loops through all specified `search_terms` to identify indices from `index[word]` for the key word-containing sentences and use them to extract these sentences from `document` into the same data structure as output by __B2__:\n",
    "```\n",
    "data['word'] = [[[i, j, k], [\"These\", \"are\", \"sentences\", \"containing\", \"the\", \"word\", \"'word'\", \".\"]],\n",
    "                ...,]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B5:Function(5/5)\n",
    "\n",
    "def fast_kwic(document, index, search_terms = {}):\n",
    "\n",
    "    data = defaultdict(def_value)\n",
    "\n",
    "    for term in search_terms:\n",
    "\n",
    "        if term in index:\n",
    "            # print('found it')\n",
    "            lst_word_indicies = index[term]\n",
    "            if len(lst_word_indicies) > 0:\n",
    "                for occurence in lst_word_indicies:\n",
    "                    i = occurence[0]\n",
    "                    j = occurence[1]\n",
    "                    k = occurence[2]\n",
    "                    sentence_words = document[i][j+1][1]\n",
    "                    tmp_lst = [occurence, sentence_words]\n",
    "                    data[term].append(tmp_lst)\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test our new function, lets test it on the same keywords as before: `Frankenstein` and `monster`. Note that the output from this sanity check should be the same as the one from **B3**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of sentences 'Frankenstein' appears in: 27\n",
      "# of sentences 'monster' appears in: 31\n",
      "\n",
      "She nursed Madame \n",
      " Frankenstein , my aunt , in her last illness , with the greatest affection \n",
      " and care and afterwards attended her own mother during a tedious \n",
      " illness , in a manner that excited the admiration of all who knew her , \n",
      " after which she again lived in my uncle 's house , where she was beloved \n",
      " by all the family .  \n",
      "\n",
      "I started from my sleep with horror ; a cold dew covered my forehead , my \n",
      " teeth chattered , and every limb became convulsed ; when , by the dim and \n",
      " yellow light of the moon , as it forced its way through the window \n",
      " shutters , I beheld the wretch -- the miserable monster whom I had \n",
      " created .  \n"
     ]
    }
   ],
   "source": [
    "# B5:SanityCheck\n",
    "\n",
    "fast_results = fast_kwic(document, index, {'Frankenstein', 'monster'})\n",
    "\n",
    "print(\"# of sentences 'Frankenstein' appears in: {}\".format(len(fast_results['Frankenstein'])))\n",
    "print(\"# of sentences 'monster' appears in: {}\".format(len(fast_results['monster'])))\n",
    "print()\n",
    "\n",
    "print(\" \".join(fast_results['Frankenstein'][7][1]))\n",
    "print()\n",
    "print(\" \".join(fast_results['monster'][0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__B6.__ _(5 pts)_ Your goal here is to modify the pre-processing in `load_book` one more time! Make a small modification to the input: `load_book(book_id, pos = True, lemma = True):`, to accept two boolean arguments, `pos` and `lemma` specifying how to identify each word as a key term. In particular, each word will now be represented in both of the `document` and `index` as a tuple: `heading = (text, tag)`, where `text` contains the `word.text` attribute from `spacy` if `lemma = False`, and `word.lemma_` attribute if `True`. Similarly, `tag` should be left empty as `\"\"` if `pos = False` and otherwise contain `word.pos_`.\n",
    "\n",
    "Note this functions output should still consist of a `document` and `index` in the same format aside from the replacement of `word` with `heading`, which will allow for the same use of output in `fast_kwic`, although more specified by the textual features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:Function(5/5)\n",
    "\n",
    "def load_book_3(book_id, pos = True, lemma = True):\n",
    "    \n",
    "    idx_paragraph = 0\n",
    "    idx_sentence = 0\n",
    "    idx_word = 0\n",
    "\n",
    "    document = []\n",
    "    index = defaultdict(def_value)\n",
    "\n",
    "    '''\n",
    "    tst_lst = [['paragraph1', ['sentence1', 'sentence2']], ['paragraph2']]\n",
    "    print(tst_lst[0])\n",
    "    print(tst_lst[0][1])\n",
    "    l = ['a', 'b', ['cc', 'dd', ['eee', 'fff']], 'g', 'h']\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        with open(\"./data/books/\" + book_id + \".txt\") as file:\n",
    "            book_data = file.read()\n",
    "            trimmed_book_data = book_data.strip()\n",
    "            paragraphs = re.split('\\n{2,}', trimmed_book_data)\n",
    "\n",
    "            for para in paragraphs:\n",
    "                para_list = []\n",
    "                doc = nlp(para)\n",
    "                para_list.append(doc.text)\n",
    "                document.append(para_list)\n",
    "\n",
    "                # sent_list = []\n",
    "                for sent in doc.sents:\n",
    "                    sent_list = []\n",
    "                    sent_list.append(sent.text)\n",
    "                    # doc_list[idx_paragraph].append(sent_list)\n",
    "\n",
    "                    word_list = []\n",
    "                    idx_word = 0\n",
    "\n",
    "                    for token in sent:\n",
    "                        if lemma:\n",
    "                            text = token.lemma_\n",
    "                        else:\n",
    "                            text = token.text\n",
    "\n",
    "                        if pos:\n",
    "                            tag = token.pos_\n",
    "                        else:\n",
    "                            tag = \"\"\n",
    "                        heading = (text, tag)\n",
    "\n",
    "                        word_list.append(heading)\n",
    "\n",
    "                        indicies_list = [idx_paragraph, idx_sentence, idx_word]\n",
    "                        index[heading].append(indicies_list)\n",
    "\n",
    "                        idx_word += 1\n",
    "\n",
    "                    sent_list.append(word_list)\n",
    "                    document[idx_paragraph].append(sent_list)\n",
    "\n",
    "                    idx_sentence += 1\n",
    "\n",
    "                idx_paragraph += 1\n",
    "                idx_sentence = 0\n",
    "\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error, file not found.\")\n",
    "        sys.exit()\n",
    "    \n",
    "    return document, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:SanityCheck\n",
    "document, index = load_book_3(\"84\", pos = True, lemma = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'NOUN'):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-27501de7477f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# B6:SanityCheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentence with ('cold', 'NOUN'):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_kwic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'NOUN'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'NOUN')})[('cold', 'NOUN')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence with ('cold', 'NOUN'):\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sequence item 0: expected str instance, tuple found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f1f1f91afdd1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# B6:SanityCheck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sentence with ('cold', 'NOUN'):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfast_kwic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearch_terms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cold'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NOUN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: sequence item 0: expected str instance, tuple found"
     ]
    }
   ],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'NOUN'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'NOUN')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function def_value at 0x7fc862fed820>, {('cold', 'NOUN'): [[[13, 2, 2], [('the', 'DET'), ('\\n', 'SPACE'), ('cold', 'NOUN'), ('be', 'AUX'), ('not', 'PART'), ('excessive', 'ADJ'), (',', 'PUNCT'), ('if', 'SCONJ'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('wrap', 'VERB'), ('in', 'ADP'), ('fur', 'NOUN'), ('--', 'PUNCT'), ('a', 'DET'), ('dress', 'NOUN'), ('which', 'DET'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('\\n', 'SPACE'), ('already', 'ADV'), ('adopt', 'VERB'), (',', 'PUNCT'), ('for', 'ADP'), ('there', 'PRON'), ('be', 'AUX'), ('a', 'DET'), ('great', 'ADJ'), ('difference', 'NOUN'), ('between', 'ADP'), ('walk', 'VERB'), ('the', 'DET'), ('\\n', 'SPACE'), ('deck', 'NOUN'), ('and', 'CCONJ'), ('remaining', 'ADJ'), ('seat', 'VERB'), ('motionless', 'NOUN'), ('for', 'ADP'), ('hour', 'NOUN'), (',', 'PUNCT'), ('when', 'ADV'), ('no', 'DET'), ('exercise', 'NOUN'), ('\\n', 'SPACE'), ('prevent', 'VERB'), ('the', 'DET'), ('blood', 'NOUN'), ('from', 'ADP'), ('actually', 'ADV'), ('freeze', 'VERB'), ('in', 'ADP'), ('-PRON-', 'DET'), ('vein', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[291, 1, 12], [('before', 'ADP'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('quit', 'VERB'), ('\\n', 'SPACE'), ('-PRON-', 'DET'), ('apartment', 'NOUN'), (',', 'PUNCT'), ('on', 'ADP'), ('a', 'DET'), ('sensation', 'NOUN'), ('of', 'ADP'), ('cold', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('cover', 'VERB'), ('-PRON-', 'PRON'), ('with', 'ADP'), ('some', 'DET'), ('\\n', 'SPACE'), ('clothe', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('these', 'DET'), ('be', 'AUX'), ('insufficient', 'ADJ'), ('to', 'PART'), ('secure', 'VERB'), ('-PRON-', 'PRON'), ('from', 'ADP'), ('the', 'DET'), ('dew', 'NOUN'), ('of', 'ADP'), ('\\n', 'SPACE'), ('night', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[384, 2, 24], [('yet', 'ADV'), ('-PRON-', 'PRON'), ('do', 'AUX'), ('\\n', 'SPACE'), ('not', 'PART'), ('heed', 'VERB'), ('the', 'DET'), ('bleakness', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('weather', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('well', 'ADV'), ('fit', 'VERB'), ('by', 'ADP'), ('-PRON-', 'DET'), ('\\n', 'SPACE'), ('conformation', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('endurance', 'NOUN'), ('of', 'ADP'), ('cold', 'NOUN'), ('than', 'SCONJ'), ('heat', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[661, 0, 16], [('as', 'SCONJ'), ('-PRON-', 'PRON'), ('still', 'ADV'), ('pursue', 'VERB'), ('-PRON-', 'DET'), ('journey', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('northward', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('snow', 'NOUN'), ('thicken', 'VERB'), ('and', 'CCONJ'), ('\\n', 'SPACE'), ('the', 'DET'), ('cold', 'NOUN'), ('increase', 'VERB'), ('in', 'ADP'), ('a', 'DET'), ('degree', 'NOUN'), ('almost', 'ADV'), ('too', 'ADV'), ('severe', 'ADJ'), ('to', 'PART'), ('support', 'VERB'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[686, 1, 1], [('the', 'DET'), ('cold', 'NOUN'), ('be', 'AUX'), ('excessive', 'ADJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('many', 'ADJ'), ('of', 'ADP'), ('\\n', 'SPACE'), ('-PRON-', 'DET'), ('unfortunate', 'ADJ'), ('comrade', 'NOUN'), ('have', 'AUX'), ('already', 'ADV'), ('find', 'VERB'), ('a', 'DET'), ('grave', 'NOUN'), ('amidst', 'ADP'), ('this', 'DET'), ('scene', 'NOUN'), ('of', 'ADP'), ('\\n', 'SPACE'), ('desolation', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]]]})\n"
     ]
    }
   ],
   "source": [
    "print(fast_kwic(document, index, search_terms = {('cold', 'NOUN')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B6:SanityCheck\n",
    "print(\"Sentence with ('cold', 'ADJ'):\")\n",
    "\" \".join(fast_kwic(document, index, search_terms = {('cold', 'ADJ')})[('cold', 'ADJ')][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<function def_value at 0x7fc862fed820>, {('cold', 'ADJ'): [[[9, 0, 22], [('-PRON-', 'PRON'), ('be', 'AUX'), ('already', 'ADV'), ('far', 'ADV'), ('north', 'ADV'), ('of', 'ADP'), ('London', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('as', 'SCONJ'), ('-PRON-', 'PRON'), ('walk', 'VERB'), ('in', 'ADP'), ('the', 'DET'), ('street', 'NOUN'), ('of', 'ADP'), ('\\n', 'SPACE'), ('Petersburgh', 'PROPN'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('feel', 'VERB'), ('a', 'DET'), ('cold', 'ADJ'), ('northern', 'ADJ'), ('breeze', 'NOUN'), ('play', 'NOUN'), ('upon', 'SCONJ'), ('-PRON-', 'DET'), ('cheek', 'NOUN'), (',', 'PUNCT'), ('which', 'DET'), ('\\n', 'SPACE'), ('brace', 'VERB'), ('-PRON-', 'DET'), ('nerve', 'NOUN'), ('and', 'CCONJ'), ('fill', 'VERB'), ('-PRON-', 'PRON'), ('with', 'ADP'), ('delight', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[12, 3, 19], [('i', 'NOUN'), ('\\n', 'SPACE'), ('accompany', 'VERB'), ('the', 'DET'), ('whale', 'NOUN'), ('-', 'PUNCT'), ('fisher', 'NOUN'), ('on', 'ADP'), ('several', 'ADJ'), ('expedition', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('North', 'PROPN'), ('Sea', 'PROPN'), (';', 'PUNCT'), ('\\n', 'SPACE'), ('-PRON-', 'PRON'), ('voluntarily', 'ADV'), ('endure', 'VERB'), ('cold', 'ADJ'), (',', 'PUNCT'), ('famine', 'NOUN'), (',', 'PUNCT'), ('thirst', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('want', 'VERB'), ('of', 'ADP'), ('sleep', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('often', 'ADV'), ('\\n', 'SPACE'), ('work', 'VERB'), ('hard', 'ADV'), ('than', 'SCONJ'), ('the', 'DET'), ('common', 'ADJ'), ('sailor', 'NOUN'), ('during', 'ADP'), ('the', 'DET'), ('day', 'NOUN'), ('and', 'CCONJ'), ('devote', 'VERB'), ('-PRON-', 'DET'), ('\\n', 'SPACE'), ('night', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('study', 'NOUN'), ('of', 'ADP'), ('mathematic', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('theory', 'NOUN'), ('of', 'ADP'), ('medicine', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('those', 'DET'), ('\\n', 'SPACE'), ('branch', 'NOUN'), ('of', 'ADP'), ('physical', 'ADJ'), ('science', 'NOUN'), ('from', 'ADP'), ('which', 'DET'), ('a', 'DET'), ('naval', 'ADJ'), ('adventurer', 'NOUN'), ('may', 'VERB'), ('derive', 'VERB'), ('\\n', 'SPACE'), ('the', 'DET'), ('great', 'ADJ'), ('practical', 'ADJ'), ('advantage', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[124, 10, 9], [('-PRON-', 'PRON'), ('start', 'VERB'), ('from', 'ADP'), ('-PRON-', 'DET'), ('sleep', 'NOUN'), ('with', 'ADP'), ('horror', 'NOUN'), (';', 'PUNCT'), ('a', 'DET'), ('cold', 'ADJ'), ('dew', 'NOUN'), ('cover', 'VERB'), ('-PRON-', 'DET'), ('forehead', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'DET'), ('\\n', 'SPACE'), ('tooth', 'NOUN'), ('chatter', 'VERB'), (',', 'PUNCT'), ('and', 'CCONJ'), ('every', 'DET'), ('limb', 'NOUN'), ('become', 'VERB'), ('convulsed', 'ADJ'), (';', 'PUNCT'), ('when', 'ADV'), (',', 'PUNCT'), ('by', 'ADP'), ('the', 'DET'), ('dim', 'NOUN'), ('and', 'CCONJ'), ('\\n', 'SPACE'), ('yellow', 'ADJ'), ('light', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('moon', 'NOUN'), (',', 'PUNCT'), ('as', 'SCONJ'), ('-PRON-', 'PRON'), ('force', 'VERB'), ('-PRON-', 'DET'), ('way', 'NOUN'), ('through', 'ADP'), ('the', 'DET'), ('window', 'NOUN'), ('\\n', 'SPACE'), ('shutter', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('beheld', 'VERB'), ('the', 'DET'), ('wretch', 'NOUN'), ('--', 'PUNCT'), ('the', 'DET'), ('miserable', 'ADJ'), ('monster', 'NOUN'), ('whom', 'PRON'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('\\n', 'SPACE'), ('create', 'VERB'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[136, 6, 7], [('-PRON-', 'PRON'), ('then', 'ADV'), ('pause', 'VERB'), (',', 'PUNCT'), ('and', 'CCONJ'), ('a', 'DET'), ('\\n', 'SPACE'), ('cold', 'ADJ'), ('shivering', 'NOUN'), ('come', 'VERB'), ('over', 'ADP'), ('-PRON-', 'PRON'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[157, 10, 7], [('-PRON-', 'PRON'), ('die', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('first', 'ADJ'), ('approach', 'NOUN'), ('of', 'ADP'), ('cold', 'ADJ'), ('weather', 'NOUN'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('at', 'ADP'), ('the', 'DET'), ('beginning', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('last', 'ADJ'), ('winter', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[201, 0, 20], [('no', 'DET'), ('one', 'NOUN'), ('can', 'VERB'), ('conceive', 'VERB'), ('the', 'DET'), ('anguish', 'NOUN'), ('-PRON-', 'PRON'), ('suffer', 'VERB'), ('during', 'ADP'), ('the', 'DET'), ('remainder', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('\\n', 'SPACE'), ('night', 'NOUN'), (',', 'PUNCT'), ('which', 'DET'), ('-PRON-', 'PRON'), ('spend', 'VERB'), (',', 'PUNCT'), ('cold', 'ADJ'), ('and', 'CCONJ'), ('wet', 'ADJ'), (',', 'PUNCT'), ('in', 'ADP'), ('the', 'DET'), ('open', 'ADJ'), ('air', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[252, 3, 5], [('and', 'CCONJ'), ('when', 'ADV'), ('-PRON-', 'PRON'), ('receive', 'VERB'), ('-PRON-', 'DET'), ('cold', 'ADJ'), ('answer', 'NOUN'), ('\\n', 'SPACE'), ('and', 'CCONJ'), ('hear', 'VERB'), ('the', 'DET'), ('harsh', 'ADJ'), (',', 'PUNCT'), ('unfeeling', 'ADJ'), ('reasoning', 'NOUN'), ('of', 'ADP'), ('these', 'DET'), ('man', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'DET'), ('purpose', 'VERB'), ('\\n', 'SPACE'), ('avowal', 'NOUN'), ('die', 'VERB'), ('away', 'ADV'), ('on', 'ADP'), ('-PRON-', 'DET'), ('lip', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[277, 2, 28], [('-PRON-', 'PRON'), ('be', 'AUX'), ('\\n', 'SPACE'), ('trouble', 'VERB'), (';', 'PUNCT'), ('a', 'DET'), ('mist', 'NOUN'), ('come', 'VERB'), ('over', 'ADP'), ('-PRON-', 'DET'), ('eye', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('-PRON-', 'PRON'), ('feel', 'VERB'), ('a', 'DET'), ('faintness', 'NOUN'), ('seize', 'VERB'), ('-PRON-', 'PRON'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('but', 'CCONJ'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('quickly', 'ADV'), ('restore', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('cold', 'ADJ'), ('gale', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('mountain', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[288, 7, 3], [('the', 'DET'), ('air', 'NOUN'), ('be', 'AUX'), ('cold', 'ADJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'DET'), ('rain', 'NOUN'), ('again', 'ADV'), ('begin', 'VERB'), ('to', 'PART'), ('descend', 'VERB'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('\\n', 'SPACE'), ('enter', 'VERB'), ('the', 'DET'), ('hut', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('fiend', 'NOUN'), ('with', 'ADP'), ('an', 'DET'), ('air', 'NOUN'), ('of', 'ADP'), ('exultation', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('with', 'ADP'), ('a', 'DET'), ('heavy', 'ADJ'), ('\\n', 'SPACE'), ('heart', 'NOUN'), ('and', 'CCONJ'), ('depressed', 'ADJ'), ('spirit', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[291, 0, 10], [('\"', 'PUNCT'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('dark', 'ADJ'), ('when', 'ADV'), ('-PRON-', 'PRON'), ('awake', 'VERB'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('feel', 'VERB'), ('cold', 'ADJ'), ('also', 'ADV'), (',', 'PUNCT'), ('and', 'CCONJ'), ('half', 'NOUN'), ('frighten', 'VERB'), (',', 'PUNCT'), ('as', 'SCONJ'), ('-PRON-', 'PRON'), ('\\n', 'SPACE'), ('be', 'AUX'), (',', 'PUNCT'), ('instinctively', 'ADV'), (',', 'PUNCT'), ('find', 'VERB'), ('-PRON-', 'PRON'), ('so', 'ADV'), ('desolate', 'ADJ'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[292, 4, 3], [('-PRON-', 'PRON'), ('be', 'AUX'), ('still', 'ADV'), ('cold', 'ADJ'), ('when', 'ADV'), ('under', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('the', 'DET'), ('tree', 'NOUN'), ('-PRON-', 'PRON'), ('find', 'VERB'), ('a', 'DET'), ('huge', 'ADJ'), ('cloak', 'NOUN'), (',', 'PUNCT'), ('with', 'ADP'), ('\\n', 'SPACE'), ('which', 'DET'), ('-PRON-', 'PRON'), ('cover', 'VERB'), ('-PRON-', 'PRON'), (',', 'PUNCT'), ('and', 'CCONJ'), ('sit', 'VERB'), ('down', 'ADP'), ('upon', 'SCONJ'), ('the', 'DET'), ('ground', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[295, 0, 9], [('\"', 'PUNCT'), ('one', 'NUM'), ('day', 'NOUN'), (',', 'PUNCT'), ('when', 'ADV'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('oppress', 'VERB'), ('by', 'ADP'), ('cold', 'ADJ'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('find', 'VERB'), ('a', 'DET'), ('fire', 'NOUN'), ('which', 'DET'), ('have', 'AUX'), ('be', 'AUX'), ('\\n', 'SPACE'), ('leave', 'VERB'), ('by', 'ADP'), ('some', 'DET'), ('wandering', 'NOUN'), ('beggar', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('be', 'AUX'), ('overcome', 'VERB'), ('with', 'ADP'), ('delight', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('\\n', 'SPACE'), ('warmth', 'NOUN'), ('-PRON-', 'PRON'), ('experience', 'VERB'), ('from', 'ADP'), ('-PRON-', 'PRON'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[297, 5, 36], [('a', 'DET'), ('great', 'ADJ'), ('fall', 'NOUN'), ('of', 'ADP'), ('snow', 'NOUN'), ('have', 'AUX'), ('take', 'VERB'), ('\\n', 'SPACE'), ('place', 'VERB'), ('the', 'DET'), ('night', 'NOUN'), ('before', 'ADV'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'DET'), ('field', 'NOUN'), ('be', 'AUX'), ('of', 'ADP'), ('one', 'NUM'), ('uniform', 'ADJ'), ('white', 'NOUN'), (';', 'PUNCT'), ('the', 'DET'), ('\\n', 'SPACE'), ('appearance', 'NOUN'), ('be', 'AUX'), ('disconsolate', 'ADJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('-PRON-', 'PRON'), ('find', 'VERB'), ('-PRON-', 'DET'), ('foot', 'NOUN'), ('chill', 'VERB'), ('by', 'ADP'), ('the', 'DET'), ('cold', 'ADJ'), ('\\n', 'SPACE')]], [[303, 8, 45], [('-PRON-', 'PRON'), ('raise', 'VERB'), ('-PRON-', 'PRON'), ('and', 'CCONJ'), ('smile', 'VERB'), ('with', 'ADP'), ('such', 'ADJ'), ('kindness', 'NOUN'), ('and', 'CCONJ'), ('affection', 'NOUN'), ('\\n', 'SPACE'), ('that', 'SCONJ'), ('-PRON-', 'PRON'), ('feel', 'VERB'), ('sensation', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('peculiar', 'ADJ'), ('and', 'CCONJ'), ('overpowering', 'ADJ'), ('nature', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('\\n', 'SPACE'), ('a', 'DET'), ('mixture', 'NOUN'), ('of', 'ADP'), ('pain', 'NOUN'), ('and', 'CCONJ'), ('pleasure', 'NOUN'), (',', 'PUNCT'), ('such', 'ADJ'), ('as', 'SCONJ'), ('-PRON-', 'PRON'), ('have', 'AUX'), ('never', 'ADV'), ('before', 'ADV'), ('experience', 'VERB'), (',', 'PUNCT'), ('\\n', 'SPACE'), ('either', 'CCONJ'), ('from', 'ADP'), ('hunger', 'NOUN'), ('or', 'CCONJ'), ('cold', 'ADJ'), (',', 'PUNCT'), ('warmth', 'NOUN'), ('or', 'CCONJ'), ('food', 'NOUN'), (';', 'PUNCT'), ('and', 'CCONJ'), ('-PRON-', 'PRON'), ('withdraw', 'VERB'), ('from', 'ADP'), ('the', 'DET'), ('\\n', 'SPACE'), ('window', 'NOUN'), (',', 'PUNCT'), ('unable', 'ADJ'), ('to', 'PART'), ('bear', 'VERB'), ('these', 'DET'), ('emotion', 'NOUN'), ('.', 'PUNCT')]], [[345, 7, 21], [('-PRON-', 'PRON'), ('be', 'AUX'), ('more', 'ADV'), ('agile', 'ADJ'), ('than', 'SCONJ'), ('-PRON-', 'PRON'), ('and', 'CCONJ'), ('could', 'VERB'), ('\\n', 'SPACE'), ('subsist', 'VERB'), ('upon', 'SCONJ'), ('coarser', 'NOUN'), ('diet', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('bear', 'VERB'), ('the', 'DET'), ('extreme', 'NOUN'), ('of', 'ADP'), ('heat', 'NOUN'), ('and', 'CCONJ'), ('cold', 'ADJ'), ('with', 'ADP'), ('\\n', 'SPACE'), ('less', 'ADJ'), ('injury', 'NOUN'), ('to', 'ADP'), ('-PRON-', 'DET'), ('frame', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'DET'), ('stature', 'NOUN'), ('far', 'ADV'), ('exceed', 'VERB'), ('-PRON-', 'PRON'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[412, 4, 1], [('the', 'DET'), ('cold', 'ADJ'), ('star', 'NOUN'), ('shine', 'VERB'), ('in', 'ADP'), ('mockery', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'DET'), ('bare', 'ADJ'), ('tree', 'NOUN'), ('\\n', 'SPACE'), ('wave', 'VERB'), ('-PRON-', 'DET'), ('branch', 'NOUN'), ('above', 'ADP'), ('-PRON-', 'PRON'), (';', 'PUNCT'), ('now', 'ADV'), ('and', 'CCONJ'), ('then', 'ADV'), ('the', 'DET'), ('sweet', 'ADJ'), ('voice', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('bird', 'NOUN'), ('\\n', 'SPACE'), ('burst', 'VERB'), ('forth', 'ADV'), ('amidst', 'ADP'), ('the', 'DET'), ('universal', 'ADJ'), ('stillness', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[513, 4, 8], [('but', 'CCONJ'), ('now', 'ADV'), ('-PRON-', 'PRON'), ('go', 'VERB'), ('to', 'ADP'), ('-PRON-', 'PRON'), ('in', 'ADP'), ('\\n', 'SPACE'), ('cold', 'ADJ'), ('blood', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('-PRON-', 'DET'), ('heart', 'NOUN'), ('often', 'ADV'), ('sicken', 'VERB'), ('at', 'ADP'), ('the', 'DET'), ('work', 'NOUN'), ('of', 'ADP'), ('-PRON-', 'DET'), ('hand', 'NOUN'), ('.', 'PUNCT')]], [[523, 4, 7], [('-PRON-', 'PRON'), ('have', 'AUX'), ('endure', 'VERB'), ('incalculable', 'ADJ'), ('fatigue', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('cold', 'ADJ'), (',', 'PUNCT'), ('and', 'CCONJ'), ('hunger', 'NOUN'), (';', 'PUNCT'), ('do', 'AUX'), ('-PRON-', 'PRON'), ('dare', 'VERB'), ('\\n', 'SPACE'), ('destroy', 'VERB'), ('-PRON-', 'DET'), ('hope', 'NOUN'), ('?', 'PUNCT'), ('\"', 'PUNCT')]], [[554, 2, 48], [('-PRON-', 'DET'), ('\\n', 'SPACE'), ('first', 'ADJ'), ('supposition', 'NOUN'), ('be', 'AUX'), ('that', 'SCONJ'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('the', 'DET'), ('corpse', 'NOUN'), ('of', 'ADP'), ('some', 'DET'), ('person', 'NOUN'), ('who', 'PRON'), ('have', 'AUX'), ('\\n', 'SPACE'), ('be', 'AUX'), ('drown', 'VERB'), ('and', 'CCONJ'), ('be', 'AUX'), ('throw', 'VERB'), ('on', 'ADP'), ('shore', 'NOUN'), ('by', 'ADP'), ('the', 'DET'), ('wave', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('on', 'ADP'), ('examination', 'NOUN'), ('\\n', 'SPACE'), ('-PRON-', 'PRON'), ('find', 'VERB'), ('that', 'SCONJ'), ('the', 'DET'), ('clothe', 'NOUN'), ('be', 'AUX'), ('not', 'PART'), ('wet', 'ADJ'), ('and', 'CCONJ'), ('even', 'ADV'), ('that', 'SCONJ'), ('the', 'DET'), ('body', 'NOUN'), ('be', 'AUX'), ('not', 'PART'), ('\\n', 'SPACE'), ('then', 'ADV'), ('cold', 'ADJ'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[557, 0, 20], [('another', 'DET'), ('woman', 'NOUN'), ('confirm', 'VERB'), ('the', 'DET'), ('account', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('fisherman', 'NOUN'), ('have', 'VERB'), ('bring', 'VERB'), ('the', 'DET'), ('\\n', 'SPACE'), ('body', 'NOUN'), ('into', 'ADP'), ('-PRON-', 'DET'), ('house', 'NOUN'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('not', 'PART'), ('cold', 'ADJ'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[659, 18, 20], [('follow', 'VERB'), ('-PRON-', 'PRON'), (';', 'PUNCT'), ('-PRON-', 'PRON'), ('seek', 'VERB'), ('the', 'DET'), ('everlasting', 'ADJ'), ('ice', 'NOUN'), ('\\n', 'SPACE'), ('of', 'ADP'), ('the', 'DET'), ('north', 'NOUN'), (',', 'PUNCT'), ('where', 'ADV'), ('-PRON-', 'PRON'), ('will', 'VERB'), ('feel', 'VERB'), ('the', 'DET'), ('misery', 'NOUN'), ('of', 'ADP'), ('cold', 'ADJ'), ('and', 'CCONJ'), ('frost', 'ADJ'), (',', 'PUNCT'), ('to', 'PART'), ('\\n', 'SPACE'), ('which', 'DET'), ('-PRON-', 'PRON'), ('be', 'AUX'), ('impassive', 'ADJ'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[664, 1, 25], [('-PRON-', 'PRON'), ('have', 'AUX'), ('escape', 'VERB'), ('-PRON-', 'PRON'), (',', 'PUNCT'), ('and', 'CCONJ'), ('-PRON-', 'PRON'), ('must', 'VERB'), ('commence', 'VERB'), ('a', 'DET'), ('destructive', 'ADJ'), ('and', 'CCONJ'), ('almost', 'ADV'), ('endless', 'ADJ'), ('\\n', 'SPACE'), ('journey', 'NOUN'), ('across', 'ADP'), ('the', 'DET'), ('mountainous', 'ADJ'), ('ice', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('ocean', 'NOUN'), (',', 'PUNCT'), ('amidst', 'ADP'), ('cold', 'ADJ'), ('that', 'DET'), ('few', 'ADJ'), ('\\n', 'SPACE'), ('of', 'ADP'), ('the', 'DET'), ('inhabitant', 'NOUN'), ('could', 'VERB'), ('long', 'ADV'), ('endure', 'VERB'), ('and', 'CCONJ'), ('which', 'DET'), ('-PRON-', 'PRON'), (',', 'PUNCT'), ('the', 'DET'), ('native', 'NOUN'), ('of', 'ADP'), ('a', 'DET'), ('\\n', 'SPACE'), ('genial', 'ADJ'), ('and', 'CCONJ'), ('sunny', 'ADJ'), ('climate', 'NOUN'), (',', 'PUNCT'), ('could', 'VERB'), ('not', 'PART'), ('hope', 'VERB'), ('to', 'PART'), ('survive', 'VERB'), ('.', 'PUNCT'), (' ', 'SPACE')]], [[689, 5, 50], [('and', 'CCONJ'), ('now', 'ADV'), (',', 'PUNCT'), ('behold', 'NOUN'), (',', 'PUNCT'), ('with', 'ADP'), ('the', 'DET'), ('first', 'ADJ'), ('imagination', 'NOUN'), ('of', 'ADP'), ('\\n', 'SPACE'), ('danger', 'NOUN'), (',', 'PUNCT'), ('or', 'CCONJ'), (',', 'PUNCT'), ('if', 'SCONJ'), ('-PRON-', 'PRON'), ('will', 'VERB'), (',', 'PUNCT'), ('the', 'DET'), ('first', 'ADJ'), ('mighty', 'ADJ'), ('and', 'CCONJ'), ('terrific', 'ADJ'), ('trial', 'NOUN'), ('of', 'ADP'), ('-PRON-', 'DET'), ('\\n', 'SPACE'), ('courage', 'NOUN'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('shrink', 'VERB'), ('away', 'ADV'), ('and', 'CCONJ'), ('be', 'AUX'), ('content', 'ADJ'), ('to', 'PART'), ('be', 'AUX'), ('hand', 'VERB'), ('down', 'ADP'), ('as', 'SCONJ'), ('man', 'NOUN'), ('who', 'PRON'), ('\\n', 'SPACE'), ('have', 'AUX'), ('not', 'PART'), ('strength', 'NOUN'), ('enough', 'ADV'), ('to', 'PART'), ('endure', 'VERB'), ('cold', 'ADJ'), ('and', 'CCONJ'), ('peril', 'NOUN'), (';', 'PUNCT'), ('and', 'CCONJ')]], [[709, 7, 2], [('-PRON-', 'PRON'), ('be', 'AUX'), ('cold', 'ADJ'), (',', 'PUNCT'), ('-PRON-', 'PRON'), ('can', 'VERB'), ('not', 'PART'), ('answer', 'VERB'), ('\\n', 'SPACE'), ('-PRON-', 'PRON'), ('.', 'PUNCT'), ('\"', 'PUNCT')]], [[717, 0, 7], [('\"', 'PUNCT'), ('there', 'ADV'), ('-PRON-', 'PRON'), ('lie', 'VERB'), (',', 'PUNCT'), ('white', 'ADJ'), ('and', 'CCONJ'), ('cold', 'ADJ'), ('in', 'ADP'), ('death', 'NOUN'), ('.', 'PUNCT'), (' ', 'SPACE')]]]})\n"
     ]
    }
   ],
   "source": [
    "print(fast_kwic(document, index, search_terms = {('cold', 'ADJ')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
