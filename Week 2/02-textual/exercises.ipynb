{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br>Chapter 2: Feature engineering and language processing\n",
    "\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.3 Exercise: Regex phone numbers\n",
    "Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['215-345-3463', '215-756-8273']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "document = open(\"./data/phone-numbers.txt\", \"r\").read()\n",
    "\n",
    "numbers = re.findall('215-[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]', \n",
    "                     document)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.8 Exercise: Names of the gods\n",
    "In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the Smith', 'the Maid', 'the Father', 'the Warrior', 'the Crone']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n",
    "\n",
    "gods = re.findall(\"the [A-Z][a-z]+\", text)\n",
    "\n",
    "gods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.4 Exercise: Improving a regex-based sentence tokenizer\n",
    "First, write a few sentences in a complex (but grammatically acceptable) way so that the (above) regex-based tokenizer breaks. Then, fix the pattern so that the tokenizer can handle your text appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"With all due resp., I don't think this is a very good tokenization!\",\n",
       " '',\n",
       " \"Here's another one!\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## regex-based sentence tokenizer\n",
    "sentences = \"With all due resp., I don't think this is a very good tokenization! Here's another one!\"\n",
    "sentences_tokenized = re.split(\"\\s*(?<=[\\.\\?\\!][^a-zA-Z0-9,])\\s*\", sentences)\n",
    "sentences_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Exercise: POS tagging \n",
    "Apply POS tagging to a sentence of your choosing and filter for only verbs and nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\tcoarse\tfine\n",
      "Use\tVERB\tVB\n",
      "test\tNOUN\tNN\n",
      "sentences\tNOUN\tNNS\n",
      "Joey\tPROPN\tNNP\n",
      "'s\tVERB\tVBZ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "running_sentence = \"Use some of our test sentences; Joey's not very smart, nor charming.\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"token\\tcoarse\\tfine\")\n",
    "for token in doc:\n",
    "    if token.pos_ in {\"NOUN\", \"VERB\", \"PROPN\"}:\n",
    "        print(token.text + \"\\t\" + token.pos_ + \"\\t\" + token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.5 Exercise: using grammar for information extraction\n",
    "Apply the spacy grammatical parsing and extract any subject-verb token pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject\tverb\n",
      "'s use\n",
      "hour is\n",
      "we meet\n"
     ]
    }
   ],
   "source": [
    "running_sentence = \"Let's use another one. Anything else? Happy hour is tomorrow at 5:30 at Tap House where we will all meet up and say hi.\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"subject\\tverb\")\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "        print(token.text + \" \"+ token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.4 Exercise: improved word frequency representation\n",
    "Build a stop word list and lemmatization strategy (potentially using POS tags) to compute 'better' word frequencies, as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BOW', 4),\n",
       " ('model', 4),\n",
       " ('word', 3),\n",
       " ('be', 3),\n",
       " ('text', 2),\n",
       " ('simply', 2),\n",
       " ('use', 2),\n",
       " ('frequency', 1),\n",
       " ('probably', 1),\n",
       " ('first', 1),\n",
       " ('easy', 1),\n",
       " ('numerical', 1),\n",
       " ('representation', 1),\n",
       " ('compute', 1),\n",
       " ('community', 1),\n",
       " ('refer', 1),\n",
       " ('bag', 1),\n",
       " ('put', 1),\n",
       " ('count', 1),\n",
       " ('number', 1),\n",
       " ('time', 1),\n",
       " ('appear', 1),\n",
       " ('document', 1),\n",
       " ('course', 1),\n",
       " ('depend', 1)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"Word frequencies are probably the first and easiest \n",
    "numerical representation of text to compute. In some communities, \n",
    "this is referred to as the bag of words (BOW) model. \n",
    "Put simply, the BOW model simply counts up the \n",
    "number of times each word appears in a document. \n",
    "This of course depends on a few things, e.g., case and lemmatization. \n",
    "However, constructing a basic BOW model is quite straightforward, especially using `Counter`. \n",
    "Let's use this very paragraph as our example text for the BOW model.\"\"\"\n",
    "\n",
    "# in addition to excluding stop words, let's also exclude specific parts of speech, like determiners, particles,\n",
    "# punctuation, and adpositions.\n",
    "\n",
    "stop_words = {'\\n', ',', '.', '`', 'the', 'and', 'of'}\n",
    "excluded_pos = {\"DET\", \"PART\", \"PUNCT\", \"ADP\"}\n",
    "\n",
    "doc = nlp(text)\n",
    "word_counts = Counter()\n",
    "\n",
    "for word in doc:\n",
    "    if word.lemma_ not in stop_words and word.pos_ not in excluded_pos:\n",
    "        word_counts[(word.lemma_)] += 1\n",
    "\n",
    "word_counts.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.5 Exercise: exploring TF-IDF\n",
    "Rank each of the example TF-IDF matrix's rows by TF-IDF values from high-to-low and interpret the kinds of words that have high TF-IDF values, i.e., are 'more important'. What about the low values, what kinds of words are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def count_words(sentence):\n",
    "    frequency = Counter()\n",
    "    for word in sentence:\n",
    "        frequency[word.text.lower()] += 1\n",
    "    return frequency\n",
    "\n",
    "text = '''Lost and weary, Catelyn Stark gave herself over to her gods. \n",
    "She knelt before the Smith, who fixed things that were broken, \n",
    "and asked that he give her sweet Bran his protection. \n",
    "She went to the Maid and beseeched her to lend her courage to Arya and Sansa, \n",
    "to guard them in their innocence. \n",
    "To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, \n",
    "and she asked the Warrior to keep Robb strong and shield him in his battles. \n",
    "Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \n",
    "\"Guide me, wise lady,\" she prayed. \n",
    "\"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "    \n",
    "## the 'master' set, keeps track of the words in all documents\n",
    "all_words = set()\n",
    "\n",
    "## store the word frequencies by book\n",
    "all_doc_frequencies = {}\n",
    "\n",
    "## loop over the sentences\n",
    "for j, sentence in enumerate(doc.sents):\n",
    "    frequency = count_words(sentence)\n",
    "    all_doc_frequencies[j] = frequency\n",
    "    doc_words = set(frequency.keys())\n",
    "    all_words = all_words.union(doc_words)\n",
    "    \n",
    "## create a matrix of zeros: (words) x (documents)\n",
    "TDM = np.zeros((len(all_words),len(all_doc_frequencies)))\n",
    "## fix a word ordering for the rows\n",
    "all_words = sorted(list(all_words))\n",
    "## loop over the (sorted) document numbers and (ordered) words; fill in matrix\n",
    "for j in all_doc_frequencies:\n",
    "    for i, word in enumerate(all_words):\n",
    "        TDM[i,j] = all_doc_frequencies[j][word]\n",
    "\n",
    "num_docs = TDM.shape[1]\n",
    "\n",
    "## start off with a copy of our TDM (frequencies)\n",
    "TFIDF = np.array(TDM)\n",
    "## loop over words\n",
    "for i, word in enumerate(all_words):\n",
    "    ## count docs containing the word\n",
    "    num_docs_containing_word = len([x for x in TDM[i] if x])\n",
    "    ### computen the inverse document frequence of this word\n",
    "    IDF = -np.log2(num_docs_containing_word/num_docs)\n",
    "    ## multiply this row by the IDF to transform it to TFIDF\n",
    "    TFIDF[i,] = TFIDF[i,]*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For document #0, words ranked according to TF-IDF are:\n",
      "\n",
      "catelyn\t2.8074\n",
      "gave\t2.8074\n",
      "gods\t2.8074\n",
      "herself\t2.8074\n",
      "lost\t2.8074\n",
      "over\t2.8074\n",
      "stark\t2.8074\n",
      "weary\t2.8074\n",
      "her\t0.8074\n",
      "to\t0.8074\n",
      "and\t0.4854\n",
      "\n",
      "For document #1, words ranked according to TF-IDF are:\n",
      "\n",
      "that\t3.6147\n",
      "before\t2.8074\n",
      "bran\t2.8074\n",
      "broken\t2.8074\n",
      "fixed\t2.8074\n",
      "give\t2.8074\n",
      "he\t2.8074\n",
      "knelt\t2.8074\n",
      "protection\t2.8074\n",
      "smith\t2.8074\n",
      "sweet\t2.8074\n",
      "things\t2.8074\n",
      "were\t2.8074\n",
      "who\t2.8074\n",
      "asked\t1.8074\n",
      "his\t1.8074\n",
      "her\t0.8074\n",
      "and\t0.4854\n",
      "she\t0.4854\n",
      "the\t0.4854\n",
      "\n",
      "For document #2, words ranked according to TF-IDF are:\n",
      "\n",
      "to\t3.2294\n",
      "arya\t2.8074\n",
      "beseeched\t2.8074\n",
      "courage\t2.8074\n",
      "guard\t2.8074\n",
      "innocence\t2.8074\n",
      "lend\t2.8074\n",
      "maid\t2.8074\n",
      "sansa\t2.8074\n",
      "their\t2.8074\n",
      "them\t2.8074\n",
      "went\t2.8074\n",
      "her\t1.6147\n",
      "and\t0.9709\n",
      "in\t0.8074\n",
      "she\t0.4854\n",
      "the\t0.4854\n",
      "\n",
      "For document #3, words ranked according to TF-IDF are:\n",
      "\n",
      "it\t5.6147\n",
      "to\t3.2294\n",
      "battles\t2.8074\n",
      "father\t2.8074\n",
      "for\t2.8074\n",
      "him\t2.8074\n",
      "justice\t2.8074\n",
      "keep\t2.8074\n",
      "know\t2.8074\n",
      "robb\t2.8074\n",
      "seek\t2.8074\n",
      "shield\t2.8074\n",
      "strength\t2.8074\n",
      "strong\t2.8074\n",
      "warrior\t2.8074\n",
      "wisdom\t2.8074\n",
      "the\t1.9417\n",
      "asked\t1.8074\n",
      "his\t1.8074\n",
      "prayed\t1.8074\n",
      "and\t1.4563\n",
      "she\t0.9709\n",
      "in\t0.8074\n",
      "\n",
      "For document #4, words ranked according to TF-IDF are:\n",
      "\n",
      "a\t2.8074\n",
      "crone\t2.8074\n",
      "hand\t2.8074\n",
      "lamp\t2.8074\n",
      "lastly\t2.8074\n",
      "often\t2.8074\n",
      "one\t2.8074\n",
      "showed\t2.8074\n",
      "statues\t2.8074\n",
      "turned\t2.8074\n",
      "whose\t2.8074\n",
      "with\t2.8074\n",
      "her\t0.8074\n",
      "in\t0.8074\n",
      "to\t0.8074\n",
      "she\t0.4854\n",
      "the\t0.4854\n",
      "\n",
      "For document #5, words ranked according to TF-IDF are:\n",
      "\n",
      "\"\t3.6147\n",
      "guide\t2.8074\n",
      "lady\t2.8074\n",
      "wise\t2.8074\n",
      "me\t1.8074\n",
      "prayed\t1.8074\n",
      "she\t0.4854\n",
      "\n",
      "For document #6, words ranked according to TF-IDF are:\n",
      "\n",
      "\"\t3.6147\n",
      "me\t3.6147\n",
      "ahead\t2.8074\n",
      "dark\t2.8074\n",
      "do\t2.8074\n",
      "i\t2.8074\n",
      "let\t2.8074\n",
      "lie\t2.8074\n",
      "must\t2.8074\n",
      "not\t2.8074\n",
      "path\t2.8074\n",
      "places\t2.8074\n",
      "show\t2.8074\n",
      "stumble\t2.8074\n",
      "walk\t2.8074\n",
      "that\t1.8074\n",
      "the\t0.9709\n",
      "in\t0.8074\n",
      "and\t0.4854\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for j in range(TFIDF.shape[1]):\n",
    "    doc_vals = TFIDF[:,j]\n",
    "    \n",
    "    # make word and TF-IDF value tuples, put them in a list, sort the list according to TF-IDF values, then only keep words with non-zero TF-IDF \n",
    "    \n",
    "    words_and_vals = [(word, val) for word, val in sorted(zip(all_words, doc_vals), key = lambda x: x[1], reverse = True) if val]\n",
    "    print(\"For document #\" + str(j) + \", words ranked according to TF-IDF are:\\n\")\n",
    "    for word, val in words_and_vals:\n",
    "        print(word + \"\\t\" + str(round(val, 4)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that words that are rare across documents have higher TF-IDF values. The lower the TF-IDF value, the more common the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
