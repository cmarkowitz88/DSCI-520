{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br>Chapter 2: Feature engineering and language processing\n",
    "\n",
    "## 2.0 Features\n",
    "\n",
    "When analyzing large amounts of data, it's important to be able to find trends and patterns. A useful tool which helps in this regard is through the use of _features_, which are created through a process usually referred to as _feature engineering_. Any specific characteristic of a data point may be used as a _feature_. \n",
    "\n",
    "Say we had a large group of people, and we wanted to create an algorithm that predicts which individuals are professional basketball players. The most immediate thing that may pop into one's head when it comes to professional basketball players is height, so we could create a height feature by measuring each person's height and recording it into our dataset. This is an example of a _numeric feature_&mdash;the focus of __Chapter 1__! These will be incredibly useful for us in the future, when we start using machine learning techniques. Of course, there are plenty of other types of non-numeric features, as well, which is the focus of this chapter! But to get started let's just look ahead (statistically) to a familiar example method of numerical feature enginerring (standardization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.1 Example: standardizing numerical features\n",
    "\n",
    "In order to avoid the impact of using different units such as inches or pounds for measured features, it's common and important for analysis to standardize data. Essentially, this is a numerical operation where if the data is denoted by $x$ and its mean and standard deviation are denoted by $\\mu$ and $\\sigma$, respectively, the standardization operation can be expressed as the following transformation:\n",
    "\n",
    "$$x_i \\mapsto \\frac{x_i - \\mu}{\\sigma}$$\n",
    "\n",
    "Mathematically, this _translates_ the _mean_ of the data to 0 via subtraction (addition) and _scales_ the _standard deviation_ to 1 via division (multiplication). We'll think more about numeric feature development in future chapters, using different measures and quantitative frameworks, but to calculate the current transformation, we only need use functions from `numpy` module (we'll use a [dataset of baseball player heights and weights](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# defining a function to perform the standardization\n",
    "\n",
    "def standardize(data):\n",
    "    mean = np.mean(data)\n",
    "    stdev = np.std(data)\n",
    "    \n",
    "    standardized_data = (data - mean) / stdev\n",
    "    \n",
    "    return standardized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Name Team       Position  Height  Weight    Age\n",
      "0    Adam_Donachie  BAL        Catcher      74   180.0  22.99\n",
      "1        Paul_Bako  BAL        Catcher      74   215.0  34.69\n",
      "2  Ramon_Hernandez  BAL        Catcher      72   210.0  30.78\n",
      "3     Kevin_Millar  BAL  First_Baseman      72   210.0  35.43\n",
      "4      Chris_Gomez  BAL  First_Baseman      73   188.0  35.71\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Team</th>\n",
       "      <th>Position</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Adam_Donachie</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Catcher</td>\n",
       "      <td>0.131344</td>\n",
       "      <td>-1.033741</td>\n",
       "      <td>-1.330806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Paul_Bako</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Catcher</td>\n",
       "      <td>0.131344</td>\n",
       "      <td>0.634409</td>\n",
       "      <td>1.378644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ramon_Hernandez</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Catcher</td>\n",
       "      <td>-0.736447</td>\n",
       "      <td>0.396102</td>\n",
       "      <td>0.473178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kevin_Millar</td>\n",
       "      <td>BAL</td>\n",
       "      <td>First_Baseman</td>\n",
       "      <td>-0.736447</td>\n",
       "      <td>0.396102</td>\n",
       "      <td>1.550011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chris_Gomez</td>\n",
       "      <td>BAL</td>\n",
       "      <td>First_Baseman</td>\n",
       "      <td>-0.302552</td>\n",
       "      <td>-0.652449</td>\n",
       "      <td>1.614852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Brian_Roberts</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Second_Baseman</td>\n",
       "      <td>-2.038133</td>\n",
       "      <td>-1.224387</td>\n",
       "      <td>0.151286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Miguel_Tejada</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Shortstop</td>\n",
       "      <td>-2.038133</td>\n",
       "      <td>0.348441</td>\n",
       "      <td>0.470863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Melvin_Mora</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Third_Baseman</td>\n",
       "      <td>-1.170343</td>\n",
       "      <td>-0.080512</td>\n",
       "      <td>1.466643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Aubrey_Huff</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Third_Baseman</td>\n",
       "      <td>0.999134</td>\n",
       "      <td>1.396992</td>\n",
       "      <td>0.336548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Adam_Stern</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>-1.170343</td>\n",
       "      <td>-1.033741</td>\n",
       "      <td>-0.390603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Jeff_Fiorentino</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>-0.302552</td>\n",
       "      <td>-0.652449</td>\n",
       "      <td>-1.124702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Freddie_Bynum</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>-0.302552</td>\n",
       "      <td>-1.033741</td>\n",
       "      <td>-0.411445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Nick_Markakis</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>0.131344</td>\n",
       "      <td>-0.795434</td>\n",
       "      <td>-1.261333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Brandon_Fahey</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>0.131344</td>\n",
       "      <td>-1.986969</td>\n",
       "      <td>-0.608286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Corey_Patterson</td>\n",
       "      <td>BAL</td>\n",
       "      <td>Outfielder</td>\n",
       "      <td>-2.038133</td>\n",
       "      <td>-1.033741</td>\n",
       "      <td>-0.274815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Name Team        Position    Height    Weight       Age\n",
       "0     Adam_Donachie  BAL         Catcher  0.131344 -1.033741 -1.330806\n",
       "1         Paul_Bako  BAL         Catcher  0.131344  0.634409  1.378644\n",
       "2   Ramon_Hernandez  BAL         Catcher -0.736447  0.396102  0.473178\n",
       "3      Kevin_Millar  BAL   First_Baseman -0.736447  0.396102  1.550011\n",
       "4       Chris_Gomez  BAL   First_Baseman -0.302552 -0.652449  1.614852\n",
       "5     Brian_Roberts  BAL  Second_Baseman -2.038133 -1.224387  0.151286\n",
       "6     Miguel_Tejada  BAL       Shortstop -2.038133  0.348441  0.470863\n",
       "7       Melvin_Mora  BAL   Third_Baseman -1.170343 -0.080512  1.466643\n",
       "8       Aubrey_Huff  BAL   Third_Baseman  0.999134  1.396992  0.336548\n",
       "9        Adam_Stern  BAL      Outfielder -1.170343 -1.033741 -0.390603\n",
       "10  Jeff_Fiorentino  BAL      Outfielder -0.302552 -0.652449 -1.124702\n",
       "11    Freddie_Bynum  BAL      Outfielder -0.302552 -1.033741 -0.411445\n",
       "12    Nick_Markakis  BAL      Outfielder  0.131344 -0.795434 -1.261333\n",
       "13    Brandon_Fahey  BAL      Outfielder  0.131344 -1.986969 -0.608286\n",
       "14  Corey_Patterson  BAL      Outfielder -2.038133 -1.033741 -0.274815"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# load some sample data of baseball player heights and weights\n",
    "baseball_data = pd.read_csv(\"./data/baseball_heightweight.csv\", header = 0)\n",
    "print(baseball_data.head())\n",
    "\n",
    "# standardizing the heights\n",
    "baseball_data[\"Height\"] = standardize(baseball_data[\"Height\"])\n",
    "\n",
    "# standardizing the weights\n",
    "baseball_data[\"Weight\"] = standardize(baseball_data[\"Weight\"])\n",
    "\n",
    "# standardizing the ages\n",
    "baseball_data[\"Age\"] = standardize(baseball_data[\"Age\"])\n",
    "\n",
    "baseball_data.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the data no longer depends on units, and rather it is expressed in a standard way that very visibly describes a player's variation from the mean. The closer a value is to 0, the closer that attribute of the player is to the average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.2 Working with unstructured data\n",
    "\n",
    "Much of the data we will run into is fairly unstructured in nature. This is where expertise in data science becomes especially essential. With the rise of social media platforms online, a huge amount of data is comprised of text, and textual data is not generally distributed or generated in neat, structured manners with a set of clearly-defined features. How can we analyze this ubiquitous textual data? As an example, if you wanted to compare two people by the text they've written it might be natural to compare their words, but how do you identify the words in a document?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0.3 Featurization\n",
    "\n",
    "The adding of structure to an unstructured text object is called _featurization_. Featurization does not only refer to a process for text, but really any curcumstance in which you might need a higher-level, more-succicnt, or refined form of data representation. For images, it's common to extract feature objects. For example, an image's objects might include cars, people, or lines of paint on the road, depending on the application. These might be represented polygons or boxed pixel regions.\n",
    "\n",
    "As it turns out, featurization in images is a relatively complex task whose study could probably comprise an entire course on its own. Since text is quite accessible as a data type, still unstructured, and is human readable, we'll study featurization in its context. Moreover, the basic methodology (regular expressions) required to featurize text is the same for fixing file malformations and other pre-processing tasks, so it may already be familiar! \n",
    "\n",
    "Featurization of textual data is a small part of the large field of Natural Language Processing (NLP), so we'll review the field a bit and then explore its tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 NLP\n",
    "\n",
    "### 2.1.0 What is NLP?\n",
    "\n",
    "As mentioned, a huge amount of the data available on the internet comes in the form of unstructured text, mostly generated by humans. Such text of strictly human origin is usually referred to as _natural language_. Thus, a simple way to describe what NLP really is is to say that it's the study of techniques which allow for the processing of natural language. In recent years, it has become one of the most important and hottest subfields of computer science. Definitely not a bad thing to familiarize yourself with! Some of the more typical problems include speech recognition (spoken word also counts as natural language!), natural language understanding (machine reading comprehension of natural language), and natural language generation (using a machine to automatically generate human-like text, think of something like Siri or Alexa)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Regular expressions\n",
    "\n",
    "Regular expressions, or regex, are \"sequences of characters that define a search pattern\", according to Wikipedia. These patterns can be used to search for, find, replace, and do a great deal more with strings.\n",
    "\n",
    "Regular expression patterns are constructed with both ordinary and special characters. The simplest regular expressions are simply ordinary characters like \"A\", or \"5\", or \"status\". These patterns only match themselves, allowing you to search for exact patterns of characters. Some characters are \"special\" for regex, like \"|\" or \"\\[\" or \"^\". These characters can be used to construct regex that is more powerful than straightforward matching.\n",
    "\n",
    "#### 2.1.1.1 Basics\n",
    "\n",
    "Python's included `re` module can be used to construct and use regular expressions. It comes with many useful functions. The most basic of match object if the pattern matched the string and a `None` value if it didn't. This means `re.search()` outputs can be used with conditional statements (like `if` statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 8), match='fish'>\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "\n",
    "silly_string = \"one fish two fish red fish blue fish\"\n",
    "print(re.search(\"fish\", silly_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(re.search(\"salmon\", silly_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fish were found.\n"
     ]
    }
   ],
   "source": [
    "if re.search(\"fish\", silly_string):\n",
    "    print(\"Fish were found.\")\n",
    "else:\n",
    "    print(\"There were no fish.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful function is `re.sub()`, which takes two patterns and a string as input and replaces the first pattern with the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silly_cats = re.sub(\"fish\", \"cat\", silly_string)\n",
    "\n",
    "print(silly_cats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`re.findall()` will return all matches of a pattern in a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.findall(\"cat\", silly_cats))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.2 A few useful character classes and other means for flexibility\n",
    "\n",
    "- `.` __(wild card)__ In the default mode, this matches any character except a newline.\n",
    "- `[...]` __(character class)__ Used to indicate flexible matching across a specificed set of characters.\n",
    "- `[^...]` __(complimentary character class)__ Used to indicate flexible matching across _everything but_ a specificed set of characters.\n",
    "- `[a-z]` __(lowercase range)__ Used to indicate flexible matching across lowercase letter ranges\n",
    "- `[A-Z]` __(uppercase range)__ Used to indicate flexible matching across uppercase letter ranges\n",
    "- `[0-9]` __(numeric range)__ Used to indicate flexible matching across numeric ranges\n",
    "- '|' __(or)__ Creates a regular expression that will match either A or B. \n",
    "\n",
    "Like the `string.split()` method, `re` also has a `re.split()` method that can be used with regex patterns. We could combine this with a character class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_a_silly_string = \"Oftentimes, different punctuation characters are used; these indicate different types of stops.\"\n",
    "\n",
    "## split a string by several types of punctuation\n",
    "clauses = re.split(\"[,;.]\", not_a_silly_string)\n",
    "print(clauses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have some text that we suspect contains Philadelphia area ZIP codes, we could use character classes to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Drexel's University City campus falls in 19104, while the Collge of Nursing is in 19102 and the Philadelphia City Hall is in 19107.\"\n",
    "\n",
    "zipcodes = re.findall(\"191[0-5][0-9]\", text) # we know philly zipcodes go from 19102 to 19154\n",
    "print(zipcodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.3 Exercise: Regex phone numbers\n",
    "Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.4 Grouping, numbered groups and extensions\n",
    "Grouping is a great way to modify and extend strings, without simply replacing them. With grouping, you can use the matched content in a substitute string. It's great for re-formatting text. Groups can also serve extended functions if they are initiated by an unescaped question mark.\n",
    "- `(...)` __(group)__ Matches whatever regular expression is inside the parentheses, and indicates the start and end of a group; the contents of a group can be retrieved after a match has been performed, and can be matched later in the string with the `\\1`, `\\2`, etc., special sequences, described below.\n",
    "- `\\1`, `\\2`, etc. __(captured groups)__ Matched groups are captured and held in order: low to high from left to right, and in the case of nested groups, from outside to inside.\n",
    "- `(?:...)` __(non-matching group)__ Matches `...` as in the parentheses, but does not capture it in a group. This becomes especially important when applying multipliers.\n",
    "- `(?=...)` __(lookahead)__ Matches if `...` matches next, but doesn’t consume any of the string.\n",
    "- `(?!...)` __(negative look ahead)__ Matches if `...` doesn’t match next.\n",
    "- `(?<=...)` __(positive look behind)__ Matches if the current position in the string is preceded by a match for `...` that ends at the current position. \n",
    "- `(?<!...)` __(negative look behind)__ Matches if the current position in the string is not preceded by a match for `...`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommy_two_tone = (\"Apparently, 867-5307 is Jenny's phone number,\"+\n",
    "                 \" but I'm not sure what her area code is.\")\n",
    "\n",
    "## let's capture Jenny's phone number and insert the area code\n",
    "modified_tommy_two_tone = re.sub(r\"([0-9][0-9][0-9]-[0-9][0-9][0-9][0-9])\",\n",
    "                                 r\"1-800-\\1\", \n",
    "                                 tommy_two_tone)\n",
    "\n",
    "print(modified_tommy_two_tone)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.5 Multipliers (quantifiers)\n",
    "It was a little bit of overkill to use the numeric character class so many times in a row in the last expression. This is an example of where multiplies can come in really handy.\n",
    "- `*` __(zero or more)__ Causes the resulting RE to match 0 or more repetitions of the preceding RE, as many repetitions as are possible. \n",
    "- `+` __(one or more)__ Causes the resulting RE to match 1 or more repetitions of the preceding RE. ab+ will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’.\n",
    "- `?` __(zero or one)__ Causes the resulting RE to match 0 or 1 repetitions of the preceding RE.\n",
    "- `{m}` __(exactly m times)__ Specifies that exactly m copies of the previous RE should be matched.\n",
    "- `{m,n}` __(m throug n times)__ Causes the resulting RE to match from m to n repetitions of the preceding RE, attempting to match as many repetitions as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's get all of the phone numbers in a string\n",
    "numbers = re.findall(\"[0-9]{3}-[0-9]{4}\", tommy_two_tone)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## capture the word that appears before a lookahead: \"'s phone number\" \n",
    "## by matching one or more non-space characters before \n",
    "## along with the number itself with flexible \"|\" matching\n",
    "whos_number = re.findall(\"([^ ]+)(?='s phone number)|([0-9]{3}-[0-9]{4})\", tommy_two_tone)\n",
    "\n",
    "print(whos_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can even get a bit more flexible with our area-code handling!\n",
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "my_contact_information = \"If you need my office line, it's 215-895-2185\"\n",
    "\n",
    "## By grouping and using a `{1,2}` flexible match, we can get full and partial numbers\n",
    "## Note: we have to use a non-capturing group in order to make sure we get the full expression\n",
    "## without capturing the first three digits, only.\n",
    "numbers =  re.findall(\"(?:[0-9]{3}-){1,2}[0-9]{4}\", tommy_two_tone)\n",
    "print(numbers)\n",
    "numbers =  re.findall(\"(?:[0-9]{3}-){1,2}[0-9]{4}\", my_contact_information)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.6 Escapes and special sequences\n",
    "As it turns out, some character classes are so common that they have their own special-characters. So, our phone-number example could be even more concise with the `\\d` special character.\n",
    "- `\\` __(escape)__ Either escapes special characters (permitting you to match characters like `*`, `?`, and so forth), or signals a special sequence.\n",
    "- `\\d` __(digits)__ Matches any Unicode decimal digit. This includes `[0-9]`, and also many other digit characters.\n",
    "- `\\D` __(digits)__ Matches any Unicode non-digit.\n",
    "- ` \\s` __(whitespace)__ Matches Unicode whitespace characters, including `[\\t\\n\\r]` and space.\n",
    "- `\\w` __(word characters)__ Matches Unicode word characters; this includes most characters that can be part of a word in any language, as well as numbers and the underscore.\n",
    "- `\\W` __(non-word characters)__ Matches Unicode non-word characters;\n",
    "- `\\t` __(tab)__ Matches a tab character.\n",
    "- `\\n` __(newline)__ matches a newline character.\n",
    "- `\\r` __(carriage return)__ matches a carriage return character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's get all of the phone numbers in a string\n",
    "numbers = re.findall(\"(?:\\d{3}){1,2}-\\d{4}\", tommy_two_tone)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tommy_two_tone = \"Apparently, 867-5307 is Jenny's phone number, but I'm not sure what her area code is.\"\n",
    "\n",
    "## let's get all of the phone numbers in a string\n",
    "numbers = re.search(\"((?:\\d{3}){1,2}-\\d{4})\", tommy_two_tone)\n",
    "print(numbers.groups())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.7 Anchors\n",
    "Anchors allow you to make the positions of matches absolute in the overally position in a string. These become especially handy if you are pre-processing semi-structured text, like a screenplay, stenographer's court record, or the index of a book.\n",
    "- `^` __(start anchor)__ Matches the start of the string.\n",
    "- `$` __(end anchor)__ Matches the end of the string or just before the newline at the end of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## an example of some sem-structured text\n",
    "macbeth = (\"First Witch: When shall we three meet again? In thunder, lightning, or in rain?\\n\"+\n",
    "           \"Second Witch: When the hurlyburly's done, when the battle's lost and won.\")\n",
    "print(macbeth)\n",
    "print(\"\")\n",
    "\n",
    "## make some empty lists for our data\n",
    "speakers = []\n",
    "speeches = []\n",
    "\n",
    "## split the document into the lines of the play\n",
    "lines = macbeth.split(\"\\n\")\n",
    "\n",
    "## loop over the lines\n",
    "for line in lines:\n",
    "    \n",
    "    ## retrieve the matched groups\n",
    "    ## Note: if we simply split by a colon \n",
    "    ## we might mess up what people are saying in the text!\n",
    "    ## Also note: the super greedy \".*?\" matching ANYTHING, zero or more times!\n",
    "    ## This comes in very handy when you want loosely anything\n",
    "    ## that happens to be surrounded by some specified structure\n",
    "    speaker, speech = re.search(\"^(.*?): (.*?)$\", line).groups()\n",
    "\n",
    "    ## Grow the lists\n",
    "    speakers.append(speaker)\n",
    "    speeches.append(speech)\n",
    "\n",
    "print(speakers)\n",
    "print(\"\")\n",
    "print(speeches)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.8 Exercise: Names of the gods\n",
    "In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n",
    "\n",
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Tokenization\n",
    "Tokenization is the process of breaking up text into smaller units. Usually, this means breaking a string up into words. \n",
    "#### 2.1.2.1 `string.split()`\n",
    "The simplest possible tokenization would be to use the `string.split()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$3.88', 'in', 'New', 'York.', 'Please', 'buy', 'me', 'two', 'of', 'them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "sentences = \"Good muffins cost $3.88\\nin New York.  Please buy me\\ntwo of them.\\n\\nThanks.\"\n",
    "\n",
    "words = sentences.split()\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem with this is, punctuation has been captured as part of some words. \n",
    "\n",
    "#### 2.1.2.2 NLTK tokenization\n",
    "For a more advanced tokenizer, we'll use one of the most well-known Python modules for natural language processing, the Natural Language Toolkit (`nltk`). (Install it with `pip3 install nltk`, then import it with `import nltk` and run `nltk.download()`, which will open up a graphical window and allow you to download the data NLTK needs to perform many tasks.) ([Docs](https://www.nltk.org/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "words = nltk.tokenize.word_tokenize(sentences)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.3 Sentence tokenization\n",
    "Clearly, tokenization becomes complex, quickly, but it's also not just for the task of separating words. Another common need in language processing is sentence tokenization. NLTK has functionality for this, too, but first let's explore a hardline strategy using only a simple regex pattern, can you write sentences that break this tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## regex-based sentence tokenizer\n",
    "sentences_tokenized = re.split(\"\\s*(?<=[\\.\\?\\!][^a-zA-Z0-9])\\s*\", sentences)\n",
    "sentences_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.4 Exercise: Improving a regex-based sentence tokenizer\n",
    "First, write a few sentences in a complex (but grammatically acceptable) way so that the (above) regex-based tokenizer breaks. Then, fix the pattern so that the tokenizer can handle your text appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.5 Sentence tokenization with NLTK\n",
    "Among many other functionalities, NLTK has a relatively-light sentence tokenizer. Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Good muffins cost $3.88\\nin New York.',\n",
       " 'Please buy me\\ntwo of them.',\n",
       " 'Thanks.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_tokenized = nltk.sent_tokenize(sentences)\n",
    "sentences_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.6 Tokenization with Spacy\n",
    "A newer set of tools can be found in the `spacy` module (`pip3 install spacy`). ([Docs](https://spacy.io/usage))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Good', 0, 0), ('muffins', 1, 5), ('cost', 2, 13), ('$', 3, 18), ('3.88', 4, 19), ('\\n', 5, 23), ('in', 6, 24), ('New', 7, 27), ('York', 8, 31), ('.', 9, 35), (' ', 10, 37), ('Please', 11, 38), ('buy', 12, 45), ('me', 13, 49), ('\\n', 14, 51), ('two', 15, 52), ('of', 16, 56), ('them', 17, 59), ('.', 18, 63), ('\\n\\n', 19, 64), ('Thanks', 20, 66), ('.', 21, 72)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "doc = nlp(sentences)\n",
    "\n",
    "# spacy creates \"token\" objects which have quite a few properties. Check the documentation out if you're interested in learning more.\n",
    "\n",
    "words = []\n",
    "\n",
    "for token in doc:\n",
    "    words.append((token.text, token.i, token.idx))\n",
    "    \n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a conda package in the current Jupyter kernel\n",
    "# import sys\n",
    "# !conda install --yes --prefix {sys.prefix} spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-2.3.5-cp38-cp38-macosx_10_9_x86_64.whl (10.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 10.2 MB 10.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy) (1.18.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy) (4.47.0)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Using cached wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: setuptools in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy) (49.2.0.post20200714)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy) (2.24.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp38-cp38-macosx_10_9_x86_64.whl (18 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp38-cp38-macosx_10_9_x86_64.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 9.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp38-cp38-macosx_10_9_x86_64.whl (31 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp38-cp38-macosx_10_9_x86_64.whl (982 kB)\n",
      "\u001b[K     |████████████████████████████████| 982 kB 4.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp38-cp38-macosx_10_9_x86_64.whl (5.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.8 MB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp38-cp38-macosx_10_9_x86_64.whl (105 kB)\n",
      "\u001b[K     |████████████████████████████████| 105 kB 6.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Installing collected packages: wasabi, murmurhash, srsly, cymem, catalogue, preshed, plac, blis, thinc, spacy\n",
      "Successfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spacy-2.3.5 srsly-1.0.5 thinc-7.4.5 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 8.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.47.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.0.post20200714)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.9)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Applications/Anaconda/anaconda3/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=381135ca8bc3882f4f0f59e0c969bd1975ca9e8376689f48b0e9801c8b860ef8\n",
      "  Stored in directory: /private/var/folders/jk/cc3j41890hg12ds4qbk4gqrc0000gn/T/pip-ephem-wheel-cache-habx5t2_/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/en_core_web_sm -->\n",
      "/Applications/Anaconda/anaconda3/lib/python3.8/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem – as you may have noticed – is that there are variations between tokenizers that can result in different outcomes for you further down the line. Choice of tokenizer can make or break a particular application. \n",
    "\n",
    "### 2.1.3 Higher-level NLP\n",
    "One very convenient (albeit somewhat heavy) aspect of Spacy is that the function (which we named `nlp()`) returned by `spacy.load(\"en\")` will actually perform _a lot_ of different language modeling operations, including part of speech (POS) tagging, lemmatization, and grammatical parsing. We'll explore these two capabilities in detail, as they can be very useful for downstream applications. However,  it's important at this point to note that language processing is in and of itself a challenging and deep field that data scientists (and many others) are engaged in. As an engineering discipline, NLP is generally broken down into tasks, like POS tagging. While systems built to satisfy tasks like lemmatization have become quite mature&mdash;to the point of being packaged into Spacy and nltk&mdash;there are a number of other tasks of interest, especially focused on semantic processing and language generation&mdash;that are very much still unresolved. Moreover, the sophisticated solutions for any of these tasks often rely on some complex modeling strategies that not only require knowedge of future chapters in this course, but should really have a full course devoted to their study.\n",
    "\n",
    "#### 2.1.3.0 Spacy's `Token` object type\n",
    "Another import aspect of Spacy and it language modeling capacity is its foundational definition of  the `token` object, which, like a `string` or `dict`, has methods and attributes. There are many, so it's best to consult the [docs](https://spacy.io/api/token), but to get us started here are a few we'll play around with:\n",
    "\n",
    "- A few important attributes:\n",
    "    - `Token.i`: The index of the token within the parent document.\n",
    "    - `Token.idx`: The character offset of the token within the parent document.\n",
    "    - `Token.head`: The syntactic parent, or \"governor\", of this token.\n",
    "    - `Token.lemma_`: Base form of the token, with no inflectional suffixes.\n",
    "    - `Token.pos_`: Coarse-grained part-of-speech.\n",
    "    - `Token.tag_`: Fine-grained part-of-speech.\n",
    "    - `Token.dep_`: Syntactic dependency relation.\n",
    "\n",
    "- A few tree-navigation methods:\n",
    "    - `Token.ancestors`: The rightmost token of this token's syntactic descendants.\n",
    "    - `Token.children`: A sequence of the token's immediate syntactic children.\n",
    "    - `Token.subtree`: A sequence of all the token's syntactic descendants.\n",
    "\n",
    "#### 2.1.3.1 Part of speech (POS) tagging\n",
    "POS tags coarsely indicate how individual (word) tokens are categorized. Example tags include 'noun', 'verb', etc., and some are harder to get right than others. For example, a word like `'run'` might get used as a noun or a verb, depending on context. It is up to the POS tagger to determine which tag is correct! Thus, POS tagging is most-commonly resolved as a supervised learning problem, where human annotators label text tokens by hand for tags. These 'gold standard' data are then used to train algorithms, like those at work inside of spacy. Let's see what Spacy can do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_sentence = \"I run around all day, even after I go for a run in the morning.\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"token\\tcoarse\\tfine\")\n",
    "for token in doc:\n",
    "    print(token.text + \"\\t\" + token.pos_ + \"\\t\" + token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Exercise: POS tagging \n",
    "Apply POS tagging to a sentence of your choosing and filter for only verbs and nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.3 Lemmatization\n",
    "Simplistically, a lemma is the (somewhat arbitrarily) form of a word chosen to head an entry in a dictionary. For example, the forms 'run', 'ran', and 'running' might have the commen lemma of 'run'. Let's see how Spacy does with this! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_sentence = \"\"\"I ran out of gas after running around all day—maybe I shouldn't go for runs in the morning anymore.\"\"\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"token\\tlemma\")\n",
    "for token in doc:\n",
    "    print(token.text + \"\\t\" + token.lemma_+ \"\\t\" + token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.4 Grammatical parsing\n",
    "Parse, or syntax trees have been am important object of study for the NLP community, focusing on the relations between words in sentences, like 'subject', 'object', etc.  While grammatical parsing has come a long way in recent years, it can only tell us so much about the meanings of words in text. However, for those with knowledge of grammar and how the different relations represent interaction between words, grammatical parsing can be a powerful tool for featurization and rule-based processing. If languages were entirely beholden to grammar, there would certainly be a lot less ambiguity for NLP to sort out!\n",
    "\n",
    "Before we get going, it's important to get a few things straight. Grammatical dependencies are encoded on _parse trees_. Generally, a verb will be at the root of such a tree, and subsequently _dependent_ words will fall below in a kind of heirarchy. This makes it important to know what a give word's parents and children are. Thus, to 'navigate' parse trees Spacy has a number of `Token` methods to generate sequences of related tokens. To understand what's going on here let's look at a relatively-simple sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_sentence = \"I like to work on NLP projects.\"\n",
    "doc = nlp(ez_sentence)\n",
    "\n",
    "print(\"token\\thead\\tchildren\")\n",
    "for token in doc:\n",
    "    print(token.text + \"\\t\" + token.head.text + \"\\t\", list(token.children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way of knowing which tokens operate on which others, we can explore what these operations (relations) are. Since this is the `token.dep_`, i.e., dependency attribute, it refers to relation between a given token and its parent(`token.head`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_sentence = \"I like to work on NLP projects.\"\n",
    "doc = nlp(ez_sentence)\n",
    "\n",
    "print(\"token\\thead\\tdependency\")\n",
    "for token in doc:\n",
    "    print(token.text + \"\\t\" + token.head.text + \"\\t\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of this information (and more) can actually be output in a single organized (JSON) format using `doc.print_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_sentence = \"I like to work on NLP projects.\"\n",
    "doc = nlp(ez_sentence)\n",
    "doc.print_tree()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in case you were wondering where Spacy's sentence tokenization is, look no further than the `doc.sents` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentences)\n",
    "list(map(str, doc.sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.5 Exercise: using grammar for information extraction\n",
    "Apply the spacy grammatical parsing and extract any subject-verb token pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4 Numeric representations of text features\n",
    "So far, we've covered some of how text is processed in various ways to construct meaningful features. However, as discussed in __Section 2.0.1__, much of the work in feature development is quantitative. Even though our textual objects to this point have been categorical&mdash;including the complex, grammatical parsing&mdash; we can still build numeric features from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.1 Word-Frequency Distributions \n",
    "Word-frequency distributions are a very common input to machine learning and statistical classifiers. For a single document, $d$, let's call $f_d(w)$ the word frequency function, which indicates the number of times the word $w$ appeared in document $d$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.2 Computing word frequencies\n",
    "Word frequencies are probably the first and easiest numerical representation of text to compute. In some communities, this is referred to as the bag of words (BOW) model. Put simply, the BOW model simply counts up the number of times each word appears in a document. This of course depends on a few things, e.g., case and lemmatization. However, constructing a basic BOW model is quite straightforward, especially using `Counter`. Let's use this very paragraph as our example text for the BOW model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "text = \"\"\"Word frequencies are probably the first and easiest \n",
    "numerical representation of text to compute. In some communities, \n",
    "this is referred to as the bag of words (BOW) model. \n",
    "Put simply, the BOW model simply counts up the \n",
    "number of times each word appears in a document. \n",
    "This of course depends on a few things, e.g., case and lemmatization. \n",
    "However, constructing a basic BOW model is quite straightforward, especially using `Counter`. \n",
    "Let's use this very paragraph as our example text for the BOW model.\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "word_counts = Counter()\n",
    "\n",
    "for word in doc:\n",
    "    word_counts[word.text] += 1\n",
    "\n",
    "word_counts.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.3 Stop words and lemmatization\n",
    "Since I had to put irregular whitespace (newlines) to make the text, it ended up attached to some of the words and we also ended up counting an empty string. However, there may generally be words that are not of interest for a BOW model, and if we want to exclude them they are called 'stop words'. It's straightforward to put together a stop word `set()` and use Python's infullness to satisfy this need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {'\\n', ',', '.', '`', 'the', 'and', 'of'}\n",
    "\n",
    "doc = nlp(text)\n",
    "word_counts = Counter()\n",
    "\n",
    "for word in doc:\n",
    "    if word.text not in stop_words:\n",
    "        word_counts[word.text] += 1\n",
    "\n",
    "word_counts.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue here is capitalization, is \"The\" a different word from \"the\"? We could solve this by lowercasing all words before counting them, too, but since we've got Spacy's full power behind us let's just use the lemmas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = {'\\n', ',', '.', '`', 'the', 'and', 'of'}\n",
    "\n",
    "doc = nlp(text)\n",
    "word_counts = Counter()\n",
    "\n",
    "for word in doc:\n",
    "    if word.lemma_ not in stop_words:\n",
    "        word_counts[word.lemma_] += 1\n",
    "\n",
    "word_counts.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.4 Exercise: improved word frequency representation\n",
    "Build a stop word list and lemmatization strategy (potentially using POS tags) to compute 'better' word frequencies, as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.5 Token-level quantifications\n",
    "While frequency gives us a number that we can attach to each wird (type) that appeared in a text, it's important to note that these frequencies lack context from the individual instances in which words appeared, i.e., are are document-level quantifications of text. There's a developing literature on token-level quantifications, called _embeddings_, or word vectors. The general idea is to model the _semantics_, i.e., meanings of tokens as points in some finite-dimensional vector space. The result (of some very complex modeling!) of such a system assignes a vector to each word in an (often fixed) vocabulary. This topic is a deep one, but fortunately for us we once again have the benefit of some funcionality through Spacy, thanks to two attributes:\n",
    "\n",
    "- `Token.has_vector`: A boolean value indicating whether a word vector is associated with the token.\n",
    "- `Token.vector`: A real-valued meaning representation.\n",
    "\n",
    "The first makes it easy for us to make sure we're not trying to access a vector that doesn't exist (some words are our of vocabulary!), and the second provides us with the vector itself, provided it exists. Let's see how this works! Using Spacy, it appears that word vectors are generally of length 384. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ez_sentence = \"I like to work on NLP projects.\"\n",
    "doc = nlp(ez_sentence)\n",
    "for word in doc:\n",
    "    if word.has_vector:\n",
    "        print(word, len(word.vector), word.vector[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.5 Working with multiple documents\n",
    "Very commonly we'll be in a scenario of multiple ($n$) documents, $d_1, \\cdots, d_n$. We can measure each document's word frequencies: $f_{d_j}(w)$, but how can we put all of this information together in a convenient way?\n",
    "\n",
    "#### 2.1.5.1 Term document matrices (TDMs)\n",
    "It'd be great to build a matrix of word frequencies that spans multiple documents, but this means we'll need to come up with a master index of all of the words in all over the documents. This means establish an ordering (for us, ASCII-sort, etc., alphabetical): $w_1, \\cdots, w_m$. Once we have this, we can define a matrix:\n",
    "\n",
    "$$\n",
    "TDM = \n",
    "\\begin{bmatrix}\n",
    "    f_{d_{1}}(w_{1}) & f_{d_{2}}(w_{1}) & \\dots  & f_{d_{n}}(w_{1}) \\\\\n",
    "    f_{d_{1}}(w_{2}) & f_{d_{2}}(w_{2}) & \\dots  & f_{d_{n}}(w_{2}) \\\\\n",
    "    \\vdots           & \\vdots           & \\ddots & \\vdots \\\\\n",
    "    f_{d_{1}}(w_{m}) & f_{d_{2}}(w_{m}) & \\dots  & f_{d_{n}}(w_{m})\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "often referred to as a _term document matrix (TDM)_. For an example, let's take the sentences from the 'Names of the gods' example (above) and call them our 'documents', building a TDM.\n",
    "\n",
    "First things first, let's put our word counting into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(sentence):\n",
    "    frequency = Counter()\n",
    "    for word in sentence:\n",
    "        frequency[word.text] += 1\n",
    "    return frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our word counting off to the side, we can proceed with the matrix construction. But as mentioned above, the first thing we need is a master index of all the words so that we can define an order. Matrices are _ordered_ arrays after all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "text = '''Lost and weary, Catelyn Stark gave herself over to her gods. \n",
    "She knelt before the Smith, who fixed things that were broken, \n",
    "and asked that he give her sweet Bran his protection. \n",
    "She went to the Maid and beseeched her to lend her courage to Arya and Sansa, \n",
    "to guard them in their innocence. \n",
    "To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, \n",
    "and she asked the Warrior to keep Robb strong and shield him in his battles. \n",
    "Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \n",
    "\"Guide me, wise lady,\" she prayed. \n",
    "\"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"\n",
    "'''\n",
    "\n",
    "doc = nlp(text)\n",
    "    \n",
    "## the 'master' set, keeps track of the words in all documents\n",
    "all_words = set()\n",
    "\n",
    "## store the word frequencies by book\n",
    "all_doc_frequencies = {}\n",
    "\n",
    "## loop over the sentences\n",
    "for j, sentence in enumerate(doc.sents):\n",
    "    frequency = count_words(sentence)\n",
    "    all_doc_frequencies[j] = frequency\n",
    "    doc_words = set(frequency.keys())\n",
    "    all_words = all_words.union(doc_words)\n",
    "    \n",
    "## create a matrix of zeros: (words) x (documents)\n",
    "TDM = np.zeros((len(all_words),len(all_doc_frequencies)))\n",
    "## fix a word ordering for the rows\n",
    "all_words = sorted(list(all_words))\n",
    "## loop over the (sorted) document numbers and (ordered) words; fill in matrix\n",
    "for j in all_doc_frequencies:\n",
    "    for i, word in enumerate(all_words):\n",
    "        TDM[i,j] = all_doc_frequencies[j][word]\n",
    "\n",
    "TDM[:10,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.5.1 Accessing TDM elements\n",
    "If we want access by words or documents, we need word and document indices. As a convenience, lists have a built in method that reports the index of the first instance of an object. Since our words in the `all_words` list are unique and ordered with our matrix rows, we can just run:\n",
    "```\n",
    "i = allwords.index(\"Moby\") \n",
    "```\n",
    "to get indices for a specific word-document combination. Here are the rows (document counts) for the words 'the' and 'she'. As we can see, the former ocurrs in just about every documents, while the later is more sparsely ocurring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(TDM[all_words.index('the'), ])\n",
    "print(TDM[all_words.index('she'), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.6 Term frequency-inverse document frequency (TF-IDF)\n",
    "TDMs (i.e., matrices) provide a powerful quantitative framework through which to view text. One important concept introduced by the TDM is the notion of a _corpus_ or collection of documents. This offers a basis for comparison, and better featurization of documents. Many examples use a variant of frequency called _term frequency-inverse document frequency (TF-IDF)_. TF-IDF still uses the frequency of a word (term) in a document:\n",
    "\n",
    "- $tf_{d_{i}}(w_{j}) = $ number of times a word appears in document.\n",
    "\n",
    "but goes beyond this to incorporate the frequency of documents containing a word:\n",
    "\n",
    "- $df_D(w_j) = $ number of documents $d$ in the collection $D$ containing $w_j$.\n",
    "\n",
    "Words like `'the'`, etc., will generally occur across most or all documents, i.e., with $D\\approx m$. The motivation behind _inverse_ document frequency is that boring words will appear like this, distributed widely across all documents. Thus, an inverse document frequency will reduce the feature-importance of such words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.1 Inverse document frequency\n",
    "What we're going to do is wind up with a measure that says _how surprising_ it is to see _this_ word appear in _that_ document $k$ times_. The standard TF-IDF procedure goes beyond inversion of document frequency and into a measurement of word-information density.  [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) does a good job laying out an intuitive justification:\n",
    "\n",
    "> The inverse document frequency is a measure of how much information the word provides, that is, whether the term is common or rare across all documents. It is the logarithmically scaled inverse fraction of the documents that contain the word, obtained by dividing the total number of documents by the number of documents containing the term, and then taking the logarithm of that quotient.\n",
    "\n",
    "In other words, the quantity known as IDF is actually the _negative logarithm of the fraction of documents that contain a word:_\n",
    "\n",
    "$$idf_D(w_j) = -\\log_2\\left(\\frac{df_D(w_j)}{N}\\right),$$\n",
    "\n",
    "where $\\frac{df_D(w_j)}{N}$ is the portion of documents containing the word.\n",
    "\n",
    "Note: we'll talk a bit more about logarithms in upcoming chapters. For now, we'll just take it for granted (like lemmatization, or grammatical parsing) that we can compute logariths using `numpy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.2 Aside: why take the negative logarithm? \n",
    "In general, the negative logarithm of a probability is a measure called _entropy_. In some circles, this quantity is called _information_, and in others it is called _suprise_! I'll generally stick to calling it entropy or suprise, and would think of it intuitively as _the smallest number of bits, i.e., $0$s and $1$s we would need to set aside to ensure there is a unique pattern&mdash;a binary encoding&mdash; for each word._\n",
    "\n",
    "This can be seen directly, because any probability, $p$, can be represented as a (negative) power of $2$, i.e., there exists some $b\\geq0$ such that $p = 2^{-b}$. As it turns out, $2^b$ is the number of possible $b$-bit selections (with replacement) from the two states of a bit: $0$ or $1$. Thus, we can interpret $p$ as one binary pattern of $0$s and $1$s from $b$ bits. With this representation we can see:\n",
    "\n",
    "$$-\\log_2(p) = -\\log_2(2^{-b}) = b,$$\n",
    "\n",
    "i.e., the logarithm produces $b$, the number of bits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.3 Putting together TF-IDF\n",
    "All together, TF-IDF is then ordinarily calculated as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "tfidf_{d_{i}}(w_{j}) & = idf_D(w_j) tf_{d_{i}}(w_{j}) \\\\\n",
    "& = -tf_{d_{i}}(w_{j})\\log_2\\left(\\frac{df_D(w_j)}{N}\\right) \\\\\n",
    "& = -\\log_2\\left(\\left[\\frac{df_D(w_j)}{N}\\right]^{tf_{d_{i}}(w_{j})}\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "While we might compute TF-IDF according to the top expression, it's important to come away with thinking of it in terms of the bottom expression. This is a probability to a power! Basically, this probability (to a power) can be viewed under an independence assumption as:\n",
    "\n",
    "- the probability that the word, $w_j$, appears in the document, $d_i$, $tf_{d_{i}}(w_{j})$-many times.\n",
    "\n",
    "But that actually means that tfidf _is_ an entropy/suprise framing in its own right, answering the question:\n",
    "\n",
    "> How surprising is it to see _this_ word appear in _that_ document $k$ times?\n",
    "\n",
    "Regardless, the negative logarithms, $tfidf_{d_{i}}(w_{j})$ all non-zero, hence _normalizable_ weights that we can use as features, normalized or not. Whether or not normalized TF-IDF probabilities (e.g., in Naïve Bayes) mean anything in (a modeling sense) is another question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.4 Example: Computing TF-IDF\n",
    "By working off of our TDM from above, we'll now have an easy time computing TF-IDF. The critical piece here that we've not yet discussed is how to compute logarithms. As mentioned this is available through numpy easily using `np.log2()` (for base 2 logarithms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = TDM.shape[1]\n",
    "\n",
    "## start off with a copy of our TDM (frequencies)\n",
    "TFIDF = np.array(TDM)\n",
    "## loop over words\n",
    "for i, word in enumerate(all_words):\n",
    "    ## count docs containing the word\n",
    "    num_docs_containing_word = len([x for x in TDM[i] if x])\n",
    "    ### computen the inverse document frequence of this word\n",
    "    IDF = -np.log2(num_docs_containing_word/num_docs)\n",
    "    ## multiply this row by the IDF to transform it to TFIDF\n",
    "    TFIDF[i,] = TFIDF[i,]*IDF\n",
    "    \n",
    "## check out the TF-IDF for 'the' and 'she'\n",
    "print(TFIDF[all_words.index('the'), ])\n",
    "print(TFIDF[all_words.index('she'), ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TDM[all_words.index('the'), ])\n",
    "print(TDM[all_words.index('she'), ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.5 Exercise: exploring TF-IDF\n",
    "Rank each of the example TF-IDF matrix's columns by TF-IDF values from high-to-low and interpret the kinds of words that have high TF-IDF values, i.e., are 'more important'. What about the low values, what kinds of words are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(zip(TFIDF[:,3], all_words), reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
