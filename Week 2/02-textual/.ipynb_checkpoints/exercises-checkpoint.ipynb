{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 521: Methods for analysis and interpretation <br>Chapter 2: Feature engineering and language processing\n",
    "\n",
    "## Exercises\n",
    "Note: numberings refer to the main notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.3 Exercise: Regex phone numbers\n",
    "Read the file `phone-numbers.txt`. It contains a phone number in each line. \\[Hint: use something like `lines = open(\"file.txt\", \"r\").readlines()`\\] Store only the phone numbers with the area code \"215\" in a list and print it out. Use regex-based pattern matching, not any other methods which occur to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['215-345-3463', '215-756-8273']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "lines = open(\"./data/phone-numbers.txt\", \"r\").readlines()\n",
    "lines\n",
    "\n",
    "numbers = []\n",
    "for l in lines:\n",
    "    if re.search('215-[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]', l):\n",
    "        numbers.append(l.strip())\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['215-345-3463', '215-756-8273']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "document = open(\"./data/phone-numbers.txt\", \"r\").read()\n",
    "\n",
    "numbers = re.findall('215-[0-9][0-9][0-9]-[0-9][0-9][0-9][0-9]', \n",
    "                     document)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1.8 Exercise: Names of the gods\n",
    "In the cell below is some text. It's an extract from [A Clash of Kings](https://www.goodreads.com/book/show/10572.A_Clash_of_Kings), specifically, about a character's prayer to some fictional gods. Use regex to extract the names of these gods. Your output should be a list that looks something like `[\"the Father\", \"the Mother\", \"the Warrior\"]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Lost and weary, Catelyn Stark gave herself over to her gods. She knelt before the Smith, who fixed things that were broken, and asked that he give her sweet Bran his protection. She went to the Maid and beseeched her to lend her courage to Arya and Sansa, to guard them in their innocence. To the Father, she prayed for justice, the strength to seek it and the wisdom to know it, and she asked the Warrior to keep Robb strong and shield him in his battles. Lastly she turned to the Crone, whose statues often showed her with a lamp in one hand. \"Guide me, wise lady,\" she prayed. \"Show me the path I must walk, and do not let me stumble in the dark places that lie ahead.\"'\n",
    "\n",
    "# code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2.4 Exercise: Improving a regex-based sentence tokenizer\n",
    "First, write a few sentences in a complex (but grammatically acceptable) way so that the (above) regex-based tokenizer breaks. Then, fix the pattern so that the tokenizer can handle your text appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"With all due resp., I don't think this is a very good tokenization!\",\n",
       " '',\n",
       " \"Here's another one!\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## regex-based sentence tokenizer\n",
    "sentences = \"With all due resp., I don't think this is a very good tokenization! Here's another one!\"\n",
    "sentences_tokenized = re.split(\"\\s*(?<=[\\.\\?\\!][^a-zA-Z0-9,])\\s*\", sentences)\n",
    "sentences_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.2 Exercise: POS tagging \n",
    "Apply POS tagging to a sentence of your choosing and filter for only verbs and nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\tcoarse\tfine\n",
      "Use\tVERB\tVB\n",
      "test\tNOUN\tNN\n",
      "sentences\tNOUN\tNNS\n",
      "Joey\tPROPN\tNNP\n",
      "'s\tVERB\tVBZ\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "running_sentence = \"Use some of our test sentences; Joey's not very smart, nor charming.\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"token\\tcoarse\\tfine\")\n",
    "for token in doc:\n",
    "    if token.pos_ in {\"NOUN\", \"VERB\", \"PROPN\"}:\n",
    "        print(token.text + \"\\t\" + token.pos_ + \"\\t\" + token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3.5 Exercise: using grammar for information extraction\n",
    "Apply the spacy grammatical parsing and extract any subject-verb token pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token\tcoarse\tfine\n",
      "'s use\n",
      "hour is\n",
      "we meet\n"
     ]
    }
   ],
   "source": [
    "running_sentence = \"Let's use another one. Anything else? Happy hour is tomorrow at 5:30 at Tap House where we will all meet up and say hi.\"\n",
    "doc = nlp(running_sentence)\n",
    "\n",
    "print(\"token\\tcoarse\\tfine\")\n",
    "for token in doc:\n",
    "    if token.dep_ == \"nsubj\" and token.head.pos_ == \"VERB\":\n",
    "        print(token.text + \" \"+ token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4.4 Exercise: improved word frequency representation\n",
    "Build a stop word list and lemmatization strategy (potentially using POS tags) to compute 'better' word frequencies, as you see fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.6.5 Exercise: exploring TF-IDF\n",
    "Rank each of the example TF-IDF matrix's columns by TF-IDF values from high-to-low and interpret the kinds of words that have high TF-IDF values, i.e., are 'more important'. What about the low values, what kinds of words are these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
